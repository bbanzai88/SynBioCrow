{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbanzai88/SynBioCrow/blob/main/SynbioCrowV001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This system is an automated end-to-end bio-retrosynthesis and design platform that integrates multiple computational tools into a unified LangGraph-based workflow. It identifies, evaluates, and simulates novel enzymatic pathways for the biosynthesis of target molecules, combining retrosynthetic search, enzyme selection, thermodynamic filtering, and metabolic simulation.\n",
        "\n",
        "Key Capabilities:\n",
        "\n",
        "*   Automated Retrosynthesis (RetroBioCat2)\n",
        "*   Generates de novo biochemical pathways from a target molecule’s SMILES.\n",
        "*   Performs Monte Carlo Tree Search (MCTS) over reaction rules to explore feasible routes.\n",
        "*   Pathway Extraction & Annotation\n",
        "*   Converts retrosynthesis outputs into structured reaction tables (substrates, products, scores).\n",
        "*   Integrates similarity metrics, prior precedents, and enzyme reaction type metadata.\n",
        "*   Thermodynamic & Ranking Modules\n",
        "*   Uses dGPredictor / eQuilibrator to estimate Gibbs free energies (ΔG′).\n",
        "*   Ranks pathways based on combined heuristic and thermodynamic feasibility scores.\n",
        "*   Enzyme Identification (Selenzyme)\n",
        "*   Predicts candidate enzymes for each step, optionally scraping Selenzyme for activity and organism data.\n",
        "*   Metabolic Simulation (COBRApy / novoStoic)\n",
        "*   Constructs toy stoichiometric models for top-ranked pathways.\n",
        "*   Runs flux balance analysis (FBA) to estimate steady-state feasibility and pathway yield.\n",
        "*   Sequence Ranking + Design of Experiments (DoE)\n",
        "*   Build / Approval / Export Workflow\n",
        "*   Produces pathway briefs, annotated CSVs/JSONs, simulation flux tables, and an export manifest for downstream integration or lab validation.\n",
        "*   List item\n",
        "\n",
        "Technical Architecture\n",
        "\n",
        "*   LangGraph orchestration: modular “nodes” for each stage (retrosynthesis, extract, thermo, rank, simulate, selenzyme, seqrank, doe, build, approve, export).\n",
        "*   Micromamba-based environment isolation ensures reproducible dependency management per run.\n",
        "*   Streaming execution & checkpointed state allow resuming or extending pipelines dynamically.\n",
        "*   Output artifacts (CSV, JSON, Markdown) capture every stage for transparency and reproducibility."
      ],
      "metadata": {
        "id": "10crctyo-3HH"
      },
      "id": "10crctyo-3HH"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Optional, TypedDict\n",
        "\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "from operator import add\n",
        "\n",
        "class DBTLState(TypedDict, total=False):\n",
        "    # constants (never “updated” again)\n",
        "    run_id: Annotated[str, \"const\"]\n",
        "    workdir: Annotated[str, \"const\"]\n",
        "    target_smiles: Annotated[str, \"const\"]\n",
        "    host: Annotated[str, \"const\"]\n",
        "\n",
        "    # normal fields (LastValue by default)\n",
        "    constraints: dict\n",
        "    status: str\n",
        "    last_node: str\n",
        "    approved: bool\n",
        "    signals: dict\n",
        "\n",
        "    # append-only logs\n",
        "    logs: Annotated[list[str], add]\n",
        "\n",
        "    # your artifacts…\n",
        "    retro_out_dir: str\n",
        "    pathways_all_csv: str\n",
        "    pathways_solved_csv: str\n",
        "    pathways_all_json: str\n",
        "    pathways_solved_json: str\n",
        "    pathway_json_map: dict\n",
        "    steps_plan_csv: str\n",
        "    ranked_csv: str\n",
        "    # etc…\n"
      ],
      "metadata": {
        "id": "uTr7HjkaMW-W"
      },
      "id": "uTr7HjkaMW-W",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install langgraph langchain_core\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def log(state, msg):\n",
        "    return { \"logs\": (state.get(\"logs\", []) + [msg]) }\n",
        "\n",
        "import os, json, shutil, subprocess, sys, textwrap, time\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "def _run(cmd, timeout=1800, env=None, cwd=None, check=True) -> subprocess.CompletedProcess:\n",
        "    \"\"\"Run a shell command with robust defaults and nice logging.\"\"\"\n",
        "    print(f\"[run] {cmd}\")\n",
        "    res = subprocess.run(\n",
        "        cmd,\n",
        "        shell=True,\n",
        "        cwd=cwd,\n",
        "        env=env or os.environ.copy(),\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=check,\n",
        "    )\n",
        "    print(res.stdout)\n",
        "    return res\n",
        "\n",
        "def _ensure_dirs(*paths):\n",
        "    for p in paths:\n",
        "        Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def retrosynthesis_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    End-to-end RetroBioCat-2 retrosynthesis node.\n",
        "    Bootstraps micromamba, installs RetroBioCat2, runs MCTS,\n",
        "    and saves pathway artifacts to workdir/retro_out.\n",
        "    \"\"\"\n",
        "    import os, subprocess, textwrap, json, shutil, time, glob\n",
        "    from pathlib import Path\n",
        "\n",
        "    def _log(s, msg):\n",
        "        s = {**s}\n",
        "        s[\"logs\"] = [*s.get(\"logs\", []), msg]\n",
        "        return s\n",
        "\n",
        "    try:\n",
        "        workdir = Path(state[\"workdir\"]).expanduser().resolve()\n",
        "        target = state.get(\"target_smiles\")\n",
        "        if not target:\n",
        "            raise ValueError(\"retrosynthesis_node: state['target_smiles'] is required\")\n",
        "\n",
        "        cons = (state.get(\"constraints\") or {})\n",
        "        max_time = int(cons.get(\"max_search_time\", 15))\n",
        "        expanders = list(cons.get(\"expanders\", [\"retrobiocat\"]))\n",
        "        max_depth = cons.get(\"max_depth\", None)\n",
        "        force_rerun = bool(cons.get(\"rerun\", False))\n",
        "\n",
        "        run_dir = workdir\n",
        "        retro_out = run_dir / \"retro_out\"\n",
        "        retro_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        mm_root = run_dir / \"micromamba\"\n",
        "        mm_bin = run_dir / \"bin\" / \"micromamba\"\n",
        "        env_pref = mm_root / \"envs\" / \"retrobiocat\"\n",
        "\n",
        "        state = _log(state, f\"retrosynthesis: workdir={run_dir}\")\n",
        "        state = _log(state, f\"retrosynthesis: target={target}\")\n",
        "\n",
        "        # ---- bootstrap micromamba ----\n",
        "        if not mm_bin.exists():\n",
        "            state = _log(state, \"[rbc2] installing micromamba locally …\")\n",
        "            run_dir.mkdir(parents=True, exist_ok=True)\n",
        "            subprocess.run(\n",
        "                f\"curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest -o {run_dir}/mm.tar.bz2\",\n",
        "                shell=True, check=True\n",
        "            )\n",
        "            subprocess.run(\n",
        "                f\"tar -xvjf {run_dir}/mm.tar.bz2 -C {run_dir} bin/micromamba\",\n",
        "                shell=True, check=True\n",
        "            )\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        env[\"MAMBA_ROOT_PREFIX\"] = str(mm_root)\n",
        "\n",
        "        # ---- create env if missing ----\n",
        "        if force_rerun and env_pref.exists():\n",
        "            shutil.rmtree(env_pref, ignore_errors=True)\n",
        "        if not env_pref.exists():\n",
        "            state = _log(state, \"[rbc2] creating env …\")\n",
        "            subprocess.run(\n",
        "                f\"{mm_bin} create -y -p {env_pref} -c conda-forge python=3.10 pip\",\n",
        "                shell=True, env=env, check=True\n",
        "            )\n",
        "\n",
        "        # ---- install RBC2 + pins ----\n",
        "        state = _log(state, \"[rbc2] installing RetroBioCat-2 + pins …\")\n",
        "        cmds = [\n",
        "            f\"{mm_bin} run -p {env_pref} python -m pip install --upgrade pip setuptools wheel\",\n",
        "            f\"{mm_bin} run -p {env_pref} python -m pip install https://github.com/willfinnigan/RetroBioCat-2/archive/refs/heads/main.zip \"\n",
        "            \"|| \"\n",
        "            f\"{mm_bin} run -p {env_pref} python -m pip install 'git+https://github.com/willfinnigan/RetroBioCat-2.git'\",\n",
        "            f\"{mm_bin} run -p {env_pref} python -m pip install 'pydantic<2' 'networkx<3' tqdm requests\",\n",
        "        ]\n",
        "        for c in cmds:\n",
        "            subprocess.run(c, shell=True, env=env, check=True)\n",
        "\n",
        "        # ---- inline Python for MCTS ----\n",
        "        py = textwrap.dedent(f\"\"\"\n",
        "            import os, json, csv\n",
        "            from pathlib import Path\n",
        "            from rbc2 import MCTS, get_expanders\n",
        "\n",
        "            retro_out = Path(r\"{retro_out}\")\n",
        "            retro_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            target_smi = r\"{target}\"\n",
        "            expanders  = get_expanders({json.dumps(expanders)})\n",
        "\n",
        "            mcts = MCTS(target_smi, expanders)\n",
        "            mcts.config.max_search_time = int({max_time})\n",
        "            {\"mcts.config.max_depth = int(%d)\" % int(max_depth) if max_depth is not None else \"\"}\n",
        "\n",
        "            print(\"[rbc2] Running MCTS …\")\n",
        "            mcts.run()\n",
        "\n",
        "            all_paths    = mcts.get_all_pathways()\n",
        "            solved_paths = mcts.get_solved_pathways()\n",
        "\n",
        "            def pathway_rows(pwy, tag):\n",
        "                rows = []\n",
        "                for i, rxn in enumerate(pwy.reactions, start=1):\n",
        "                    rows.append({{\n",
        "                        \"pathway_tag\": tag,\n",
        "                        \"step_idx\": i,\n",
        "                        \"reaction_smiles\": rxn.reaction_smiles(),\n",
        "                        \"product\": rxn.product,\n",
        "                        \"substrates\": \" . \".join(rxn.substrates),\n",
        "                        \"rxn_type\": rxn.rxn_type,\n",
        "                        \"name\": rxn.name,\n",
        "                        \"score\": rxn.score,\n",
        "                    }})\n",
        "                return rows\n",
        "\n",
        "            def write_bundle(paths, prefix):\n",
        "                import pandas as pd\n",
        "                all_rows = []\n",
        "                tag_to_file = {{}}\n",
        "                for k, pwy in enumerate(paths, start=1):\n",
        "                    tag = f\"P{{k:03d}}\"\n",
        "                    pjson = retro_out / f\"{{tag}}.json\"\n",
        "                    with open(pjson, \"w\", encoding=\"utf-8\") as f:\n",
        "                        json.dump(pwy.save(), f, indent=2)\n",
        "                    tag_to_file[tag] = str(pjson)\n",
        "                    all_rows.extend(pathway_rows(pwy, tag))\n",
        "                if all_rows:\n",
        "                    df = pd.DataFrame(all_rows)\n",
        "                    df.to_csv(retro_out / f\"pathways_{{prefix}}_steps.csv\", index=False)\n",
        "                    with open(retro_out / f\"pathways_{{prefix}}_steps.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                        json.dump(all_rows, f, indent=2)\n",
        "                return tag_to_file\n",
        "\n",
        "            tag_map_all = write_bundle(all_paths, \"all\")\n",
        "            tag_map_sol = write_bundle(solved_paths, \"solved\")\n",
        "\n",
        "            print(f\"[rbc2] Total pathways: {{len(all_paths)}} | Solved: {{len(solved_paths)}}\")\n",
        "            print(\"[rbc2] Outputs written to:\", retro_out)\n",
        "        \"\"\").strip()\n",
        "\n",
        "        t0 = time.time()\n",
        "        subprocess.run(\n",
        "            f\"\"\"{mm_bin} run -p {env_pref} python - <<'PY'\\n{py}\\nPY\"\"\",\n",
        "            shell=True, env=env, check=True\n",
        "        )\n",
        "        dt = time.time() - t0\n",
        "        state = _log(state, f\"[rbc2] finished in {dt:.1f}s\")\n",
        "\n",
        "        # ---- collect outputs ----\n",
        "        pathways_all_csv = str(retro_out / \"pathways_all_steps.csv\")\n",
        "        pathways_solved_csv = str(retro_out / \"pathways_solved_steps.csv\")\n",
        "        pathways_all_json = str(retro_out / \"pathways_all_steps.json\")\n",
        "        pathways_solved_json = str(retro_out / \"pathways_solved_steps.json\")\n",
        "\n",
        "        pjson_files = glob.glob(str(retro_out / \"P*.json\"))\n",
        "        tag_map = {Path(p).stem: str(Path(p)) for p in pjson_files}\n",
        "\n",
        "        new_state = {\n",
        "            **state,\n",
        "            \"retro_out_dir\": str(retro_out),\n",
        "            \"pathways_all_csv\": pathways_all_csv if os.path.exists(pathways_all_csv) else None,\n",
        "            \"pathways_solved_csv\": pathways_solved_csv if os.path.exists(pathways_solved_csv) else None,\n",
        "            \"pathways_all_json\": pathways_all_json if os.path.exists(pathways_all_json) else None,\n",
        "            \"pathways_solved_json\": pathways_solved_json if os.path.exists(pathways_solved_json) else None,\n",
        "            \"pathway_json_map\": tag_map,\n",
        "        }\n",
        "        new_state[\"logs\"] = [*new_state.get(\"logs\", []), \"retrosynthesis done\"]\n",
        "        return new_state\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        err = f\"retrosynthesis_node: subprocess failed (code={e.returncode})\"\n",
        "        new_state = {**state, \"error\": err}\n",
        "        new_state[\"logs\"] = [*new_state.get(\"logs\", []), err, str(e)]\n",
        "        return new_state\n",
        "\n",
        "    except Exception as e:\n",
        "        err = f\"retrosynthesis_node: {type(e).__name__}: {e}\"\n",
        "        new_state = {**state, \"error\": err}\n",
        "        new_state[\"logs\"] = [*new_state.get(\"logs\", []), err]\n",
        "        return new_state\n",
        "\n",
        "\n",
        "\n",
        "def extract_pathways_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Build enzyme-planning artifacts from RetroBioCat-2 outputs:\n",
        "      - steps_enzyme_plan.csv (per-step details incl. enzyme classes, precedents, Selenzyme links)\n",
        "      - pathways_ranked_no_thermo.csv (RBC2 + precedent-based ranking; no thermodynamics)\n",
        "    Reads:\n",
        "      - {workdir}/retro_out/pathways_solved_steps.csv (or .../pathways_all_steps.csv)\n",
        "      - {workdir}/retro_out/Pxxx.json (per-pathway, if present)\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    retro_out = workdir / \"retro_out\"\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ---- pick CSV (solved preferred, else all) ----\n",
        "    csv_solved = retro_out / \"pathways_solved_steps.csv\"\n",
        "    csv_all    = retro_out / \"pathways_all_steps.csv\"\n",
        "    csv_in = csv_solved if csv_solved.exists() else csv_all\n",
        "    if not csv_in.exists():\n",
        "        raise FileNotFoundError(f\"No pathways CSV found under {retro_out}\")\n",
        "\n",
        "    steps_df = pd.read_csv(csv_in)\n",
        "\n",
        "    # ---- pathway -> json map (prefer state, else scan) ----\n",
        "    json_map: Dict[str,str] = dict(state.get(\"pathway_json_map\") or {})\n",
        "    if not json_map:\n",
        "        for p in glob.glob(str(retro_out / \"P*.json\")):\n",
        "            tag = Path(p).stem  # P001\n",
        "            json_map[tag] = p\n",
        "\n",
        "    host_org = state.get(\"host\") or \"Escherichia coli\"\n",
        "\n",
        "    def selenzyme_url_from_rxn(rxn_smiles: str, org: str) -> str:\n",
        "        return (\n",
        "            \"https://selenzyme.synbiochem.co.uk/selenzy/selenzy?reaction_smiles=\"\n",
        "            + quote_plus(rxn_smiles)\n",
        "            + \"&organism=\"\n",
        "            + quote_plus(org)\n",
        "        )\n",
        "\n",
        "    # ---- expand per-step details from Pxxx.json ----\n",
        "    records = []\n",
        "    for _, r in steps_df.iterrows():\n",
        "        tag   = str(r.get(\"pathway_tag\"))\n",
        "        idx   = int(r.get(\"step_idx\"))\n",
        "        rxn   = str(r.get(\"reaction_smiles\",\"\"))\n",
        "        prod  = str(r.get(\"products\", r.get(\"product\",\"\")))\n",
        "        subs  = str(r.get(\"substrates\",\"\"))\n",
        "\n",
        "        rxn_type = r.get(\"rxn_type\",\"\")\n",
        "        name     = r.get(\"name\",\"\")\n",
        "        rbc2_score = r.get(\"score\", float(\"nan\"))\n",
        "\n",
        "        possible_enzymes = []\n",
        "        enzyme_choices = []\n",
        "        selected_enzyme = \"\"\n",
        "        precedents = []\n",
        "\n",
        "        pj = json_map.get(tag)\n",
        "        if pj and os.path.exists(pj):\n",
        "            try:\n",
        "                data = json.load(open(pj, \"r\"))\n",
        "                if isinstance(data, list) and 1 <= idx <= len(data):\n",
        "                    d = data[idx - 1]\n",
        "                    rxn_type = d.get(\"rxn_type\", rxn_type)\n",
        "                    name     = d.get(\"name\", name)\n",
        "                    tm = d.get(\"template_metadata\", {}) or {}\n",
        "                    if tm:\n",
        "                        tkey = next(iter(tm.keys()))\n",
        "                        tmeta = tm.get(tkey, {}) or {}\n",
        "                        possible_enzymes = (tmeta.get(\"possible_enzymes\") or [])\n",
        "                        choices = (tmeta.get(\"enzyme_choices\") or [])\n",
        "                        enzyme_choices = [\n",
        "                            \" / \".join(x) if isinstance(x, (list, tuple)) else str(x)\n",
        "                            for x in choices\n",
        "                        ]\n",
        "                        selected_enzyme = tmeta.get(\"selected_enzyme\") or \"\"\n",
        "                    precedents = d.get(\"precedents\") or []\n",
        "            except Exception:\n",
        "                pass  # keep whatever we already have\n",
        "\n",
        "        # top-3 precedent summary\n",
        "        prec_sorted = sorted(precedents, key=lambda x: x.get(\"similarity\", 0), reverse=True)[:3]\n",
        "        prec_summ, best_sim = [], float(\"nan\")\n",
        "        for p in prec_sorted:\n",
        "            sim = p.get(\"similarity\", float(\"nan\"))\n",
        "            dat = p.get(\"data\", {}) or {}\n",
        "            doi = dat.get(\"html_doi\", \"\") or dat.get(\"doi\", \"\")\n",
        "            enz = dat.get(\"enzyme_name\", \"\") or p.get(\"name\", \"\")\n",
        "            cite = dat.get(\"short_citation\", \"\")\n",
        "            try:\n",
        "                prec_summ.append(f\"{enz} | sim={float(sim):.3f} | {cite} | {doi}\")\n",
        "            except Exception:\n",
        "                prec_summ.append(f\"{enz} | sim={sim} | {cite} | {doi}\")\n",
        "        if prec_sorted:\n",
        "            best_sim = prec_sorted[0].get(\"similarity\", float(\"nan\"))\n",
        "\n",
        "        records.append({\n",
        "            \"pathway_tag\": tag,\n",
        "            \"step_idx\": idx,\n",
        "            \"rxn_type\": rxn_type,\n",
        "            \"retrobiocat_reaction\": name,\n",
        "            \"reaction_smiles\": rxn,\n",
        "            \"substrates\": subs,\n",
        "            \"products\": prod,\n",
        "            \"rbc2_score\": rbc2_score,\n",
        "            \"selected_enzyme\": selected_enzyme,\n",
        "            \"possible_enzymes\": \" ; \".join(possible_enzymes),\n",
        "            \"enzyme_choices\": \" ; \".join(enzyme_choices),\n",
        "            \"precedent_top3\": \" || \".join(prec_summ),\n",
        "            \"precedent_best_similarity\": best_sim,\n",
        "            \"selenzyme_url\": selenzyme_url_from_rxn(rxn, host_org),\n",
        "        })\n",
        "\n",
        "    plan = pd.DataFrame(records).sort_values([\"pathway_tag\",\"step_idx\"]).reset_index(drop=True)\n",
        "\n",
        "    # ---- pathway ranking (no thermodynamics) ----\n",
        "    tmp = plan.copy()\n",
        "    tmp[\"rbc2_score\"] = pd.to_numeric(tmp[\"rbc2_score\"], errors=\"coerce\").fillna(0.0)\n",
        "    tmp[\"precedent_best_similarity\"] = pd.to_numeric(tmp[\"precedent_best_similarity\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    agg = tmp.groupby(\"pathway_tag\").agg(\n",
        "        steps_count=(\"step_idx\",\"max\"),\n",
        "        sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "        sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    # heuristic: favor higher RBC2 + stronger precedents + fewer steps\n",
        "    agg[\"rank_score\"] = agg[\"sum_rbc2\"] + 0.2*agg[\"sum_prec_sim\"] - 0.1*agg[\"steps_count\"]\n",
        "    ranked = agg.sort_values(\n",
        "        [\"rank_score\",\"sum_rbc2\",\"sum_prec_sim\",\"steps_count\"],\n",
        "        ascending=[False, False, False, True]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # ---- write outputs ----\n",
        "    steps_plan_csv = str(finish_out / \"steps_enzyme_plan.csv\")\n",
        "    ranked_csv     = str(finish_out / \"pathways_ranked_no_thermo.csv\")\n",
        "    plan.to_csv(steps_plan_csv, index=False)\n",
        "    ranked.to_csv(ranked_csv, index=False)\n",
        "\n",
        "    # ---- return updated state ----\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"steps_plan_csv\": steps_plan_csv,\n",
        "        \"ranked_csv\": ranked_csv,\n",
        "        \"thermo_method\": \"none\",\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"extracted pathways ({len(plan)} step rows, {len(ranked)} pathways)\",\n",
        "        f\"steps_plan={steps_plan_csv}\",\n",
        "        f\"ranked_no_thermo={ranked_csv}\",\n",
        "    ]\n",
        "    return new_state\n",
        "import os, subprocess, json\n",
        "from pathlib import Path\n",
        "import textwrap\n",
        "\n",
        "def thermo_score_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Lightweight 'match-only' thermo pass:\n",
        "      - No ChemAxon, no ΔG′° numbers (columns are NaN/False).\n",
        "      - Uses the SAME micromamba env created by retrosynthesis_node.\n",
        "      - Emits:\n",
        "          retro_finish_out/steps_annotated.csv\n",
        "          retro_finish_out/pathways_ranked.csv\n",
        "      - Sets thermo_method='matchonly' and logs coverage like your working run.\n",
        "    \"\"\"\n",
        "    workdir    = Path(state[\"workdir\"])\n",
        "    mm_root    = workdir / \"micromamba\"\n",
        "    env_prefix = mm_root / \"envs\" / \"retrobiocat\"  # same env as retrosynthesis_node\n",
        "    retro_out  = workdir / \"retro_out\"\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # choose input table (prefer extractor output)\n",
        "    steps_plan_csv = state.get(\"steps_plan_csv\")\n",
        "    if not steps_plan_csv or not Path(steps_plan_csv).exists():\n",
        "        # fall back to solved/all from retro_out\n",
        "        csv_solved = retro_out / \"pathways_solved_steps.csv\"\n",
        "        csv_all    = retro_out / \"pathways_all_steps.csv\"\n",
        "        steps_plan_csv = str(csv_solved if csv_solved.exists() else csv_all)\n",
        "\n",
        "    # outputs\n",
        "    steps_out = str(finish_out / \"steps_annotated.csv\")\n",
        "    rank_out  = str(finish_out / \"pathways_ranked.csv\")\n",
        "\n",
        "    # python payload (runs INSIDE the retrobiocat env)\n",
        "    py = f\"\"\"\\\n",
        "import os, math, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "IN  = Path(r\"{steps_plan_csv}\")\n",
        "OUT_STEPS = Path(r\"{steps_out}\")\n",
        "OUT_RANK  = Path(r\"{rank_out}\")\n",
        "\n",
        "if not IN.exists():\n",
        "    raise FileNotFoundError(f\"Input steps table not found: {{IN}}\")\n",
        "\n",
        "df = pd.read_csv(IN)\n",
        "\n",
        "# Ensure required columns exist (from extractor), or create dummies\n",
        "for col in [\"pathway_tag\",\"step_idx\",\"reaction_smiles\",\"substrates\",\"products\",\n",
        "            \"rbc2_score\",\"precedent_best_similarity\",\"selenzyme_url\"]:\n",
        "    if col not in df.columns:\n",
        "        df[col] = \"\" if col not in [\"rbc2_score\",\"precedent_best_similarity\"] else 0.0\n",
        "\n",
        "# --- Parse reaction_smiles to count participants (match stats only) ---\n",
        "def parse_rxn(r):\n",
        "    if not isinstance(r, str) or \">>\" not in r: return [], []\n",
        "    L,R = r.split(\">>\",1)\n",
        "    subs=[s for s in L.split(\".\") if s]; prods=[p for p in R.split(\".\") if p]\n",
        "    return subs, prods\n",
        "\n",
        "participants = 0\n",
        "matched = 0   # 'match-only' mode → we don't do DB lookup; keep 0 like your working run\n",
        "subs_counts, prod_counts = [], []\n",
        "for rxn in df[\"reaction_smiles\"].astype(str):\n",
        "    subs, prods = parse_rxn(rxn)\n",
        "    subs_counts.append(len(subs))\n",
        "    prod_counts.append(len(prods))\n",
        "    participants += len(subs) + len(prods)\n",
        "\n",
        "df[\"subs_count\"] = subs_counts\n",
        "df[\"prods_count\"] = prod_counts\n",
        "\n",
        "# --- Thermo placeholders (no ChemAxon / no ΔG numbers) ---\n",
        "df[\"dGprime_kJ_per_mol\"] = float(\"nan\")\n",
        "df[\"uncert_kJ_per_mol\"]  = float(\"nan\")\n",
        "df[\"thermo_pass\"]        = False\n",
        "df[\"equilibrator_formula\"] = \"\"  # keep blank; earlier 'coco:...' strings were info-only\n",
        "\n",
        "# Save annotated steps\n",
        "df.to_csv(OUT_STEPS, index=False)\n",
        "\n",
        "# --- Ranking (identical heuristic to extractor) ---\n",
        "tmp = df.copy()\n",
        "tmp[\"rbc2_score\"] = pd.to_numeric(tmp[\"rbc2_score\"], errors=\"coerce\").fillna(0.0)\n",
        "tmp[\"precedent_best_similarity\"] = pd.to_numeric(tmp[\"precedent_best_similarity\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "agg = tmp.groupby(\"pathway_tag\").agg(\n",
        "    steps_count=(\"step_idx\",\"max\"),\n",
        "    sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "    sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        ").reset_index()\n",
        "\n",
        "agg[\"rank_score\"] = agg[\"sum_rbc2\"] + 0.2*agg[\"sum_prec_sim\"] - 0.1*agg[\"steps_count\"]\n",
        "ranked = agg.sort_values([\"rank_score\",\"sum_rbc2\",\"sum_prec_sim\",\"steps_count\"],\n",
        "                         ascending=[False,False,False,True]).reset_index(drop=True)\n",
        "ranked.to_csv(OUT_RANK, index=False)\n",
        "\n",
        "print(\"Match stats:\", {{\"participants\": participants, \"matched\": matched,\n",
        "                       \"steps_full_match\": 0, \"total_steps\": int(df.shape[0])}})\n",
        "print(\"Saved:\")\n",
        "print(\" \", OUT_STEPS)\n",
        "print(\" \", OUT_RANK)\n",
        "\"\"\"\n",
        "\n",
        "    # run inside the existing env\n",
        "    micromamba = workdir / \"bin\" / \"micromamba\"\n",
        "    if not micromamba.exists():\n",
        "        # In case node is called standalone, try system micromamba\n",
        "        micromamba = \"micromamba\"\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"MAMBA_ROOT_PREFIX\"] = str(mm_root)\n",
        "\n",
        "    # pandas is already in the env from earlier steps, but be forgiving:\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            f\"\"\"{micromamba} run -p \"{env_prefix}\" python - <<'PY'\\n{py}\\nPY\"\"\",\n",
        "            shell=True, check=True, text=True, env=env\n",
        "        )\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # As a fallback, try running with system Python (Colab) if env missing.\n",
        "        subprocess.run(f\"python - <<'PY'\\n{py}\\nPY\", shell=True, check=True, text=True)\n",
        "\n",
        "    # compute coverage flag (we reported matched=0 in-script)\n",
        "    coverage = 0\n",
        "    method = \"matchonly\"\n",
        "\n",
        "    # update state\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"thermo_method\": method,\n",
        "        \"steps_annotated_csv\": steps_out,\n",
        "        \"ranked_csv\": rank_out,   # keep same key as extractor (downstream nodes can reuse)\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"thermo: {method}, coverage={coverage}\",\n",
        "        f\"steps_annotated={steps_out}\",\n",
        "        f\"ranked={rank_out}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def rank_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Merge thermo info (if available) into pathway ranking.\n",
        "    Ranking logic:\n",
        "      - If thermo columns present and any thermo_pass=True:\n",
        "          prefer pathways with all_steps_pass=True, lower sum_dGprime\n",
        "      - Else fall back to rbc2_score + precedent_best_similarity - step penalty\n",
        "    Outputs:\n",
        "      {workdir}/retro_finish_out/pathways_ranked_final.csv\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Input sources\n",
        "    steps_csv = state.get(\"steps_annotated_csv\") or state.get(\"steps_plan_csv\")\n",
        "    if not steps_csv or not Path(steps_csv).exists():\n",
        "        raise FileNotFoundError(\"rank_node: steps CSV not found in state\")\n",
        "\n",
        "    df = pd.read_csv(steps_csv)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # --- detect thermo presence ---\n",
        "    has_thermo = any(c in df.columns for c in [\"dGprime_kJ_per_mol\", \"thermo_pass\"])\n",
        "    thermo_mode = bool(has_thermo and df[\"thermo_pass\"].any())\n",
        "\n",
        "    # --- normalize numeric fields ---\n",
        "    df[\"rbc2_score\"] = pd.to_numeric(df.get(\"rbc2_score\", 0.0), errors=\"coerce\").fillna(0.0)\n",
        "    df[\"precedent_best_similarity\"] = pd.to_numeric(df.get(\"precedent_best_similarity\", 0.0),\n",
        "                                                    errors=\"coerce\").fillna(0.0)\n",
        "    if \"dGprime_kJ_per_mol\" in df:\n",
        "        df[\"dGprime_kJ_per_mol\"] = pd.to_numeric(df[\"dGprime_kJ_per_mol\"], errors=\"coerce\")\n",
        "\n",
        "    # --- aggregate by pathway ---\n",
        "    group = df.groupby(\"pathway_tag\", dropna=True)\n",
        "\n",
        "    if thermo_mode:\n",
        "        # Include thermo-based weighting\n",
        "        ranked = (\n",
        "            group.agg(\n",
        "                steps_count=(\"step_idx\",\"max\"),\n",
        "                sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "                sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        "                thermo_pass_steps=(\"thermo_pass\",\"sum\"),\n",
        "                sum_dGprime=(\"dGprime_kJ_per_mol\",\"sum\"),\n",
        "            )\n",
        "            .reset_index()\n",
        "        )\n",
        "        ranked[\"all_steps_pass\"] = ranked[\"thermo_pass_steps\"] == ranked[\"steps_count\"]\n",
        "        ranked[\"rank_score\"] = (\n",
        "            ranked[\"sum_rbc2\"]\n",
        "            + 0.2 * ranked[\"sum_prec_sim\"]\n",
        "            - 0.1 * ranked[\"steps_count\"]\n",
        "            - 0.001 * ranked[\"sum_dGprime\"].fillna(0.0)\n",
        "            + ranked[\"all_steps_pass\"].astype(float) * 0.5\n",
        "        )\n",
        "        method = \"rbc2+precedent+thermo\"\n",
        "    else:\n",
        "        # Fallback: RBC2 + precedents + shortness\n",
        "        ranked = (\n",
        "            group.agg(\n",
        "                steps_count=(\"step_idx\",\"max\"),\n",
        "                sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "                sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        "            )\n",
        "            .reset_index()\n",
        "        )\n",
        "        ranked[\"rank_score\"] = (\n",
        "            ranked[\"sum_rbc2\"] + 0.2 * ranked[\"sum_prec_sim\"] - 0.1 * ranked[\"steps_count\"]\n",
        "        )\n",
        "        method = \"rbc2+precedent\"\n",
        "\n",
        "    ranked = ranked.sort_values(\n",
        "        [\"rank_score\",\"sum_rbc2\",\"sum_prec_sim\",\"steps_count\"],\n",
        "        ascending=[False, False, False, True],\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # --- save output ---\n",
        "    out_csv = str(finish_out / \"pathways_ranked_final.csv\")\n",
        "    ranked.to_csv(out_csv, index=False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"ranked_final_csv\": out_csv,\n",
        "        \"ranking_method\": method,\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"ranked pathways ({method}, {len(ranked)} entries)\",\n",
        "        f\"ranked_final={out_csv}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "import os, time, re, json\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "def selenzyme_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Collect Selenzyme links from the step table and (optionally) scrape top sequence\n",
        "    suggestions from each page. Robust to offline/blocked environments — will fall back\n",
        "    to counting links only.\n",
        "\n",
        "    Inputs (preferred to fallback):\n",
        "      - steps_annotated_csv (from thermo node) OR steps_plan_csv (from extractor)\n",
        "\n",
        "    Outputs:\n",
        "      - {workdir}/retro_finish_out/selenzyme_scrape.csv   (only if rows were scraped)\n",
        "      - state[\"selenzyme_rows\"] = number of scraped sequence rows (0 if none)\n",
        "      - state[\"scraped_sequences_csv\"] = path or None\n",
        "      - log entries\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ---- choose input table ----\n",
        "    steps_csv = state.get(\"steps_annotated_csv\") or state.get(\"steps_plan_csv\")\n",
        "    if not steps_csv or not Path(steps_csv).exists():\n",
        "        raise FileNotFoundError(\"selenzyme_node: no steps CSV found in state\")\n",
        "\n",
        "    steps = pd.read_csv(steps_csv)\n",
        "    if \"selenzyme_url\" not in steps.columns:\n",
        "        # nothing to do — return graceful \"0 rows\"\n",
        "        new_state = {\n",
        "            **state,\n",
        "            \"selenzyme_rows\": 0,\n",
        "            \"scraped_sequences_csv\": None,\n",
        "        }\n",
        "        new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "            \"selenzyme: no 'selenzyme_url' column; skipped\"\n",
        "        ]\n",
        "        return new_state\n",
        "\n",
        "    # ---- dedupe URLs, keep mapping back to step for provenance ----\n",
        "    url_map: Dict[str, List[Tuple[str,int]]] = {}\n",
        "    for _, r in steps.iterrows():\n",
        "        url = str(r.get(\"selenzyme_url\") or \"\").strip()\n",
        "        if not url:\n",
        "            continue\n",
        "        tag = str(r.get(\"pathway_tag\"))\n",
        "        idx = int(r.get(\"step_idx\", 0))\n",
        "        url_map.setdefault(url, []).append((tag, idx))\n",
        "\n",
        "    unique_urls = list(url_map.keys())\n",
        "\n",
        "    # ---- scraping controls (polite defaults) ----\n",
        "    cfg = (state.get(\"constraints\", {}) or {}).get(\"selenzyme\", {}) or {}\n",
        "    do_scrape: bool   = bool(cfg.get(\"scrape\", True))        # default True\n",
        "    max_urls: int     = int(cfg.get(\"max_urls\", 40))         # cap the number of hits\n",
        "    timeout_s: int    = int(cfg.get(\"timeout_s\", 15))\n",
        "    sleep_s: float    = float(cfg.get(\"sleep_s\", 1.0))\n",
        "\n",
        "    # limit how many we actually hit (politeness / speed)\n",
        "    urls_to_hit = unique_urls[:max_urls]\n",
        "\n",
        "    scraped_rows: List[Dict] = []\n",
        "    blocked_reason: Optional[str] = None\n",
        "\n",
        "    def _try_scrape(url: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Very light HTML scraper for Selenzyme tables. We keep this tolerant:\n",
        "        - Works even if structure changes (regex fallbacks).\n",
        "        - Returns empty list if blocked / unexpected layout.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Lazy import so node works without requests/bs4 installed\n",
        "            import requests\n",
        "            from bs4 import BeautifulSoup  # type: ignore\n",
        "\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) \"\n",
        "                              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                              \"Chrome/122.0 Safari/537.36\",\n",
        "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "                \"Connection\": \"keep-alive\",\n",
        "            }\n",
        "            resp = requests.get(url, headers=headers, timeout=timeout_s)\n",
        "            if resp.status_code in (403, 429):\n",
        "                raise RuntimeError(f\"HTTP {resp.status_code}\")\n",
        "            if not resp.ok or not resp.text:\n",
        "                return []\n",
        "            html = resp.text\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            # Look for a table with sequence hits. Selenzyme often renders a main hits table.\n",
        "            # We search broadly: any table with headers that mention UniProt/Organism/Score/EC/Name.\n",
        "            table = None\n",
        "            for t in soup.find_all(\"table\"):\n",
        "                ths = [th.get_text(strip=True).lower() for th in t.find_all(\"th\")]\n",
        "                joined = \" \".join(ths)\n",
        "                if any(k in joined for k in [\"uniprot\", \"organism\", \"score\", \"ec\", \"enzyme\"]):\n",
        "                    table = t\n",
        "                    break\n",
        "\n",
        "            rows_local: List[Dict] = []\n",
        "            if table:\n",
        "                # Extract header positions\n",
        "                headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
        "                # Build index map (case-insensitive)\n",
        "                idx_map = {h.lower(): i for i, h in enumerate(headers)}\n",
        "\n",
        "                def _get(td_list, key_variants):\n",
        "                    for k in key_variants:\n",
        "                        i = idx_map.get(k)\n",
        "                        if i is not None and i < len(td_list):\n",
        "                            return td_list[i].get_text(strip=True)\n",
        "                    return \"\"\n",
        "\n",
        "                for tr in table.find_all(\"tr\"):\n",
        "                    tds = tr.find_all(\"td\")\n",
        "                    if not tds:\n",
        "                        continue\n",
        "                    # Try a few common header variants\n",
        "                    acc = _get(tds, [\"uniprot\", \"accession\", \"uniprot id\"])\n",
        "                    org = _get(tds, [\"organism\", \"source\", \"species\"])\n",
        "                    scr = _get(tds, [\"score\", \"similarity\", \"sim\"])\n",
        "                    ec  = _get(tds, [\"ec\", \"ec number\"])\n",
        "                    enz = _get(tds, [\"enzyme\", \"name\", \"protein\"])\n",
        "\n",
        "                    # Clean accession (extract from anchor if present)\n",
        "                    if not acc:\n",
        "                        a = tds[0].find(\"a\")\n",
        "                        if a and a.get_text(strip=True):\n",
        "                            acc = a.get_text(strip=True)\n",
        "                    # Try a regex if still empty\n",
        "                    if not acc:\n",
        "                        m = re.search(r\"[A-NR-Z0-9]{6,10}\", tr.get_text(\" \", strip=True))\n",
        "                        if m:\n",
        "                            acc = m.group(0)\n",
        "\n",
        "                    # Normalize score to float if possible\n",
        "                    try:\n",
        "                        scr_val = float(re.sub(\"[^0-9.+-eE]\", \"\", scr)) if scr else None\n",
        "                    except Exception:\n",
        "                        scr_val = None\n",
        "\n",
        "                    rows_local.append({\n",
        "                        \"accession\": acc,\n",
        "                        \"organism\": org,\n",
        "                        \"score\": scr_val if scr_val is not None else scr,\n",
        "                        \"ec\": ec,\n",
        "                        \"enzyme_name\": enz,\n",
        "                    })\n",
        "\n",
        "            # If table not found, do a broad regex fallback to pick UniProt-like codes.\n",
        "            if not rows_local:\n",
        "                # extract accession-like tokens to avoid returning nothing\n",
        "                tokens = re.findall(r\"\\b[OPQ][0-9][A-Z0-9]{3}[0-9]\\b|\\b[A-NR-Z0-9]{6}\\b\", html)\n",
        "                rows_local = [{\"accession\": t, \"organism\": \"\", \"score\": \"\", \"ec\": \"\", \"enzyme_name\": \"\"}\n",
        "                              for t in dict.fromkeys(tokens)]  # dedupe preserve order\n",
        "\n",
        "            return rows_local\n",
        "\n",
        "        except Exception as e:\n",
        "            nonlocal blocked_reason\n",
        "            blocked_reason = str(e)\n",
        "            return []\n",
        "\n",
        "    if do_scrape and len(urls_to_hit) > 0:\n",
        "        for i, url in enumerate(urls_to_hit, 1):\n",
        "            rows_local = _try_scrape(url)\n",
        "            # Attach provenance (all steps that used this URL)\n",
        "            for (tag, idx) in url_map.get(url, []):\n",
        "                for r in rows_local:\n",
        "                    scraped_rows.append({\n",
        "                        \"pathway_tag\": tag,\n",
        "                        \"step_idx\": idx,\n",
        "                        \"selenzyme_url\": url,\n",
        "                        **r\n",
        "                    })\n",
        "            # polite delay\n",
        "            if i < len(urls_to_hit):\n",
        "                try:\n",
        "                    time.sleep(sleep_s)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # ---- Write output if we have any rows ----\n",
        "    scraped_csv_path: Optional[str] = None\n",
        "    if scraped_rows:\n",
        "        out_csv = finish_out / \"selenzyme_scrape.csv\"\n",
        "        pd.DataFrame(scraped_rows, columns=[\n",
        "            \"pathway_tag\",\"step_idx\",\"selenzyme_url\",\n",
        "            \"accession\",\"organism\",\"score\",\"ec\",\"enzyme_name\"\n",
        "        ]).to_csv(out_csv, index=False)\n",
        "        scraped_csv_path = str(out_csv)\n",
        "\n",
        "    # ---- Compose state & logs ----\n",
        "    total_links = len(unique_urls)\n",
        "    rows = len(scraped_rows)\n",
        "\n",
        "    log_lines = [\n",
        "        f\"selenzyme: links={total_links}, scraped_rows={rows}, \"\n",
        "        f\"mode={'scrape' if do_scrape else 'count-only'}\"\n",
        "    ]\n",
        "    if blocked_reason and rows == 0 and do_scrape:\n",
        "        log_lines.append(f\"selenzyme: scrape blocked or failed ({blocked_reason}); fell back to count-only\")\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"selenzyme_rows\": rows,\n",
        "        \"scraped_sequences_csv\": scraped_csv_path,\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + log_lines\n",
        "    return new_state\n",
        "\n",
        "import os, re, math\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "import pandas as pd\n",
        "\n",
        "def sequence_rank_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Rank Selenzyme-scraped sequences per step:\n",
        "      - Deduplicate by UniProt accession (keep best-scoring row)\n",
        "      - Prioritize bacterial organisms and preferred hosts\n",
        "      - Keep top-N per step (configurable)\n",
        "      - Emit a global shortlist (union of all per-step top-N; de-duped)\n",
        "\n",
        "    Inputs (from state):\n",
        "      - scraped_sequences_csv : path from selenzyme_node (required)\n",
        "      - steps_plan_csv        : optional; used only for sorting/validation\n",
        "\n",
        "    Constraints (optional):\n",
        "      state[\"constraints\"][\"sequence_rank\"] = {\n",
        "         \"top_n_per_step\": 5,\n",
        "         \"preferred_hosts\": [\"Escherichia coli\", \"Bacillus subtilis\"],\n",
        "         \"blacklist_regex\": \"(mitochondrial|chloroplast)\",\n",
        "         \"extra_bacterial_genera\": [\"Pseudomonas\",\"Corynebacterium\",\"Rhodococcus\"]\n",
        "      }\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    scraped_csv = state.get(\"scraped_sequences_csv\")\n",
        "    if not scraped_csv or not Path(scraped_csv).exists():\n",
        "        # graceful no-op\n",
        "        new_state = { **state }\n",
        "        new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "            \"sequence_rank: no scraped_sequences_csv; skipped\"\n",
        "        ]\n",
        "        return new_state\n",
        "\n",
        "    steps_csv = state.get(\"steps_plan_csv\")\n",
        "    steps_order = None\n",
        "    if steps_csv and Path(steps_csv).exists():\n",
        "        try:\n",
        "            _steps = pd.read_csv(steps_csv)\n",
        "            steps_order = (\n",
        "                _steps[[\"pathway_tag\",\"step_idx\"]]\n",
        "                .drop_duplicates()\n",
        "                .sort_values([\"pathway_tag\",\"step_idx\"])\n",
        "            )\n",
        "        except Exception:\n",
        "            steps_order = None\n",
        "\n",
        "    cfg: Dict = (state.get(\"constraints\", {}) or {}).get(\"sequence_rank\", {}) or {}\n",
        "    TOP_N = int(cfg.get(\"top_n_per_step\", 5))\n",
        "    preferred_hosts = set(cfg.get(\"preferred_hosts\", []))\n",
        "    extra_bact = set(cfg.get(\"extra_bacterial_genera\", [\"Pseudomonas\",\"Corynebacterium\",\"Rhodococcus\",\"Acinetobacter\"]))\n",
        "    blacklist_re = re.compile(cfg.get(\"blacklist_regex\", r\"(mitochondrial|chloroplast)\"), flags=re.I)\n",
        "\n",
        "    # --- Load scraped rows ---\n",
        "    df = pd.read_csv(scraped_csv)\n",
        "    # Normalize expected columns\n",
        "    for col in [\"pathway_tag\",\"step_idx\",\"selenzyme_url\",\"accession\",\"organism\",\"score\",\"ec\",\"enzyme_name\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = \"\" if col not in [\"score\",\"step_idx\"] else (0.0 if col==\"score\" else 0)\n",
        "\n",
        "    # Make sure types are sane\n",
        "    df[\"step_idx\"] = pd.to_numeric(df[\"step_idx\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    # score may be non-numeric (strings) → parse to float when possible\n",
        "    def _to_float(x):\n",
        "      try:\n",
        "        if pd.isna(x): return math.nan\n",
        "        return float(str(x).strip())\n",
        "      except Exception:\n",
        "        return math.nan\n",
        "    df[\"score_num\"] = df[\"score\"].apply(_to_float)\n",
        "\n",
        "    # --- Heuristics ---\n",
        "    # 1) Bacterial vs non-bacterial guess\n",
        "    # crude but effective: mark as bacteria if organism matches common bacterial genera\n",
        "    bacterial_keywords = [\n",
        "        \"bacter\", \"bacillus\",\"escherichia\",\"pseudomonas\",\"corynebacterium\",\"rhodococcus\",\n",
        "        \"acinetobacter\",\"enterobacter\",\"salmonella\",\"shigella\",\"staphylococcus\",\"streptococcus\",\n",
        "        \"lactobacillus\",\"lactococcus\",\"klebsiella\",\"burkholderia\",\"clostridium\",\"vibrio\",\n",
        "        \"shewanella\",\"xanthomonas\",\"mycobacterium\",\"yersinia\",\"pasteurella\",\"azotobacter\",\n",
        "        \"cupriavidus\",\"caulobacter\",\"alcaligenes\",\"sphingomonas\",\"terranovum\"\n",
        "    ] + [g.lower() for g in extra_bact]\n",
        "    euk_exclude = [\n",
        "        \"homo\",\"mus\",\"rattus\",\"danio\",\"drosophila\",\"bos\",\"gallus\",\"arabidopsis\",\"oryza\",\"zea\",\n",
        "        \"nicotiana\",\"saccharomyces\",\"pichia\",\"kluyveromyces\",\"aspergillus\",\"neurospora\",\"candida\"\n",
        "    ]\n",
        "\n",
        "    def is_bacterial(org: str) -> bool:\n",
        "        org_l = (org or \"\").lower()\n",
        "        if any(k in org_l for k in euk_exclude):\n",
        "            return False\n",
        "        if any(k in org_l for k in bacterial_keywords):\n",
        "            return True\n",
        "        # default unknowns to False (conservative)\n",
        "        return False\n",
        "\n",
        "    df[\"is_bacterial\"] = df[\"organism\"].apply(is_bacterial)\n",
        "\n",
        "    # 2) Preferred host bonus (exact contains)\n",
        "    def in_preferred(org: str) -> bool:\n",
        "        if not preferred_hosts: return False\n",
        "        for ph in preferred_hosts:\n",
        "            if ph.lower() in (org or \"\").lower():\n",
        "                return True\n",
        "        return False\n",
        "    df[\"is_preferred_host\"] = df[\"organism\"].apply(in_preferred)\n",
        "\n",
        "    # 3) Blacklist penalty\n",
        "    df[\"is_blacklisted\"] = df[\"organism\"].apply(lambda x: bool(blacklist_re.search(str(x))) if str(x) else False)\n",
        "\n",
        "    # 4) Accession quality hint (Swiss-Prot often 6 chars; but we won't assume)\n",
        "    def acc_quality(acc: str) -> int:\n",
        "        s = (acc or \"\").strip()\n",
        "        # basic sanity: 6-10 alphanum looks like a proper code\n",
        "        return 1 if re.fullmatch(r\"[A-NR-Z0-9]{6,10}\", s) else 0\n",
        "    df[\"acc_quality\"] = df[\"accession\"].apply(acc_quality)\n",
        "\n",
        "    # --- Composite score ---\n",
        "    # Start with normalized numeric score in [0,1]; if NaN, default 0.5\n",
        "    s = df[\"score_num\"]\n",
        "    # normalize by per-step distribution when present, else by global\n",
        "    if s.notna().sum() > 0:\n",
        "        # per-step min-max normalization (robust when distributions differ)\n",
        "        def per_step_norm(g):\n",
        "            vals = g[\"score_num\"]\n",
        "            vmin = vals.min(skipna=True)\n",
        "            vmax = vals.max(skipna=True)\n",
        "            if pd.isna(vmin) or pd.isna(vmax) or vmin == vmax:\n",
        "                return pd.Series([0.5]*len(g), index=g.index)\n",
        "            return (vals - vmin) / (vmax - vmin)\n",
        "        df[\"score_norm\"] = df.groupby([\"pathway_tag\",\"step_idx\"], dropna=False, as_index=False).apply(per_step_norm).reset_index(level=[0,1], drop=True)\n",
        "        df[\"score_norm\"] = df[\"score_norm\"].fillna(0.5)\n",
        "    else:\n",
        "        df[\"score_norm\"] = 0.5\n",
        "\n",
        "    # bonuses/penalties\n",
        "    df[\"bonus_bacterial\"] = df[\"is_bacterial\"].astype(float) * 0.20\n",
        "    df[\"bonus_preferred\"] = df[\"is_preferred_host\"].astype(float) * 0.10\n",
        "    df[\"bonus_acc\"]       = df[\"acc_quality\"].astype(float) * 0.05\n",
        "    df[\"pen_blacklist\"]   = df[\"is_blacklisted\"].astype(float) * 0.30  # big penalty\n",
        "\n",
        "    df[\"rank_score\"] = df[\"score_norm\"] + df[\"bonus_bacterial\"] + df[\"bonus_preferred\"] + df[\"bonus_acc\"] - df[\"pen_blacklist\"]\n",
        "\n",
        "    # --- Deduplicate by accession (keep best row overall) ---\n",
        "    df = df.sort_values([\"accession\",\"rank_score\"], ascending=[True, False])\n",
        "    df_dedup = df.drop_duplicates(subset=[\"accession\"], keep=\"first\")\n",
        "\n",
        "    # --- Rank within each step & keep top-N ---\n",
        "    top_by_step = (\n",
        "        df_dedup\n",
        "        .sort_values([\"pathway_tag\",\"step_idx\",\"rank_score\"], ascending=[True, True, False])\n",
        "        .groupby([\"pathway_tag\",\"step_idx\"], dropna=False)\n",
        "        .head(TOP_N)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Add UniProt URL\n",
        "    def uniprot_url(acc: str) -> str:\n",
        "        acc = str(acc).strip()\n",
        "        return f\"https://www.uniprot.org/uniprotkb/{acc}\" if acc else \"\"\n",
        "    top_by_step[\"uniprot_url\"] = top_by_step[\"accession\"].apply(uniprot_url)\n",
        "\n",
        "    # Reorder columns nicely\n",
        "    cols = [\n",
        "        \"pathway_tag\",\"step_idx\",\"accession\",\"organism\",\"ec\",\"enzyme_name\",\n",
        "        \"score\",\"score_num\",\"score_norm\",\n",
        "        \"is_bacterial\",\"is_preferred_host\",\"is_blacklisted\",\"acc_quality\",\n",
        "        \"bonus_bacterial\",\"bonus_preferred\",\"bonus_acc\",\"pen_blacklist\",\n",
        "        \"rank_score\",\"selenzyme_url\",\"uniprot_url\"\n",
        "    ]\n",
        "    cols = [c for c in cols if c in top_by_step.columns]  # guard\n",
        "    top_by_step = top_by_step[cols]\n",
        "\n",
        "    # Keep a global shortlist (unique accessions across all steps, highest rank first)\n",
        "    shortlist = (\n",
        "        top_by_step\n",
        "        .sort_values([\"rank_score\"], ascending=False)\n",
        "        .drop_duplicates(subset=[\"accession\"], keep=\"first\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Optional: preserve step ordering if we had steps_order\n",
        "    if steps_order is not None and not steps_order.empty:\n",
        "        top_by_step = top_by_step.merge(\n",
        "            steps_order.assign(_order=range(1, len(steps_order)+1)),\n",
        "            on=[\"pathway_tag\",\"step_idx\"], how=\"left\"\n",
        "        ).sort_values([\"_order\",\"rank_score\"], ascending=[True, False]).drop(columns=[\"_order\"]).reset_index(drop=True)\n",
        "\n",
        "    # --- Save outputs ---\n",
        "    ranked_csv    = finish_out / \"sequences_ranked_by_step.csv\"\n",
        "    shortlist_csv = finish_out / \"sequences_shortlist.csv\"\n",
        "    top_by_step.to_csv(ranked_csv, index=False)\n",
        "    shortlist.to_csv(shortlist_csv, index=False)\n",
        "\n",
        "    # --- Compose state ---\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"sequences_ranked_csv\": str(ranked_csv),\n",
        "        \"sequences_shortlist_csv\": str(shortlist_csv),\n",
        "        \"sequence_rank_method\": \"score_norm + bacteria + preferred_host + acc_quality - blacklist\",\n",
        "        \"sequence_rank_counts\": {\n",
        "            \"steps\": int(top_by_step[[\"pathway_tag\",\"step_idx\"]].drop_duplicates().shape[0]),\n",
        "            \"unique_accessions\": int(shortlist.shape[0]),\n",
        "            \"rows_emitted\": int(top_by_step.shape[0]),\n",
        "            \"top_n_per_step\": TOP_N,\n",
        "        }\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"sequence_rank: ranked {top_by_step.shape[0]} rows across {new_state['sequence_rank_counts']['steps']} steps\",\n",
        "        f\"sequence_rank: unique shortlisted accessions = {shortlist.shape[0]}\",\n",
        "        f\"sequence_rank: outputs={ranked_csv}, {shortlist_csv}\"\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "    import os, pandas as pd\n",
        "from pathlib import Path\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "def doe_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Build a Design-of-Experiments (DoE) screening sheet + markdown brief\n",
        "    for the top-ranked pathway, and (if available) auto-fill per-step\n",
        "    sequence fields from sequence_rank_node output.\n",
        "\n",
        "    Inputs (from state):\n",
        "      - ranked_csv or ranked_final_csv\n",
        "      - steps_plan_csv or steps_annotated_csv\n",
        "      - sequences_ranked_csv  (optional; produced by sequence_rank_node)\n",
        "\n",
        "    Outputs:\n",
        "      - {workdir}/retro_finish_out/enzyme_screening_sheet.csv\n",
        "      - {workdir}/retro_finish_out/pathway_brief.md\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # --- Inputs ---\n",
        "    rank_csv  = state.get(\"ranked_csv\") or state.get(\"ranked_final_csv\")\n",
        "    steps_csv = (\n",
        "        state.get(\"steps_plan_csv\")\n",
        "        or state.get(\"steps_annotated_csv\")\n",
        "        or str(finish_out / \"steps_enzyme_plan.csv\")\n",
        "    )\n",
        "    if not (rank_csv and Path(rank_csv).exists()):\n",
        "        raise FileNotFoundError(\"doe_node: ranked_csv missing or not found.\")\n",
        "    if not (steps_csv and Path(steps_csv).exists()):\n",
        "        raise FileNotFoundError(\"doe_node: steps CSV missing or not found.\")\n",
        "\n",
        "    ranked = pd.read_csv(rank_csv)\n",
        "    steps  = pd.read_csv(steps_csv)\n",
        "    ranked.columns = [c.strip() for c in ranked.columns]\n",
        "    steps.columns  = [c.strip() for c in steps.columns]\n",
        "\n",
        "    # --- Pick top pathway ---\n",
        "    best_tag = str(ranked.iloc[0][\"pathway_tag\"])\n",
        "    psteps = (\n",
        "        steps[steps[\"pathway_tag\"] == best_tag]\n",
        "        .sort_values(\"step_idx\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # --- Optional: bring in top-ranked sequence per step ---\n",
        "    seq_csv = state.get(\"sequences_ranked_csv\")\n",
        "    merged_n = 0\n",
        "    top_seq_by_step = None\n",
        "    if seq_csv and Path(seq_csv).exists():\n",
        "        seq = pd.read_csv(seq_csv)\n",
        "        seq.columns = [c.strip() for c in seq.columns]\n",
        "        # filter to our best pathway; pick top-1 per step by rank_score\n",
        "        if \"pathway_tag\" in seq.columns and \"rank_score\" in seq.columns:\n",
        "            seq_best = seq[seq[\"pathway_tag\"] == best_tag].copy()\n",
        "            if not seq_best.empty:\n",
        "                seq_best[\"rank_score\"] = pd.to_numeric(seq_best[\"rank_score\"], errors=\"coerce\").fillna(0.0)\n",
        "                seq_best = (\n",
        "                    seq_best.sort_values([\"step_idx\",\"rank_score\"], ascending=[True, False])\n",
        "                            .groupby(\"step_idx\", as_index=False)\n",
        "                            .first()\n",
        "                )\n",
        "                # keep tidy columns\n",
        "                keep_cols = {\n",
        "                    \"step_idx\":\"step_idx\",\n",
        "                    \"accession\":\"sequence_accession\",\n",
        "                    \"organism\":\"source_organism\",\n",
        "                    \"uniprot_url\":\"uniprot_url\"\n",
        "                }\n",
        "                for k in keep_cols.keys():\n",
        "                    if k not in seq_best.columns:\n",
        "                        seq_best[k] = \"\"\n",
        "                top_seq_by_step = seq_best[list(keep_cols.keys())].rename(columns=keep_cols)\n",
        "\n",
        "    # --- Build screening sheet skeleton ---\n",
        "    cols = [\n",
        "        \"pathway_tag\",\"step_idx\",\"retrobiocat_reaction\",\"reaction_smiles\",\n",
        "        \"selected_enzyme\",\"possible_enzymes\",\"selenzyme_url\",\n",
        "        \"sequence_accession\",\"source_organism\",\"uniprot_url\",\n",
        "        \"assay_buffer\",\"pH\",\"temp_C\",\"cofactor\",\"cofactor_recycle\",\n",
        "        \"substrate_conc_mM\",\"enzyme_loading_mg_per_mL\",\"time_h\",\n",
        "        \"conversion_%\",\"notes\"\n",
        "    ]\n",
        "    rows = []\n",
        "    for _, r in psteps.iterrows():\n",
        "        query = quote_plus(str(r.get(\"selected_enzyme\") or r.get(\"retrobiocat_reaction\")))\n",
        "        uniprot_search = f\"https://www.uniprot.org/uniprotkb?query={query}\"\n",
        "        rows.append({\n",
        "            \"pathway_tag\": best_tag,\n",
        "            \"step_idx\": int(r.get(\"step_idx\", 0)),\n",
        "            \"retrobiocat_reaction\": r.get(\"retrobiocat_reaction\", \"\"),\n",
        "            \"reaction_smiles\": r.get(\"reaction_smiles\", \"\"),\n",
        "            \"selected_enzyme\": r.get(\"selected_enzyme\", \"\"),\n",
        "            \"possible_enzymes\": r.get(\"possible_enzymes\", \"\"),\n",
        "            \"selenzyme_url\": r.get(\"selenzyme_url\", \"\"),\n",
        "            # placeholders (may be overwritten by sequence merge)\n",
        "            \"sequence_accession\": \"\",\n",
        "            \"source_organism\": \"\",\n",
        "            \"uniprot_url\": uniprot_search,\n",
        "            # DoE fields for lab to fill\n",
        "            \"assay_buffer\": \"\",\n",
        "            \"pH\": \"\",\n",
        "            \"temp_C\": \"\",\n",
        "            \"cofactor\": \"\",\n",
        "            \"cofactor_recycle\": \"\",\n",
        "            \"substrate_conc_mM\": \"\",\n",
        "            \"enzyme_loading_mg_per_mL\": \"\",\n",
        "            \"time_h\": \"\",\n",
        "            \"conversion_%\": \"\",\n",
        "            \"notes\": \"\",\n",
        "        })\n",
        "    df_screen = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "    # Merge top sequence per step (if available)\n",
        "    if top_seq_by_step is not None and not top_seq_by_step.empty:\n",
        "        before_na = df_screen[\"sequence_accession\"].isna().sum() + (df_screen[\"sequence_accession\"] == \"\").sum()\n",
        "        df_screen = df_screen.merge(top_seq_by_step, on=\"step_idx\", how=\"left\", suffixes=(\"\",\"_best\"))\n",
        "        # prefer explicit accession from ranking; otherwise keep existing\n",
        "        for col in [\"sequence_accession\",\"source_organism\",\"uniprot_url\"]:\n",
        "            best_col = f\"{col}_best\"\n",
        "            if best_col in df_screen.columns:\n",
        "                df_screen[col] = df_screen[best_col].where(df_screen[best_col].notna() & (df_screen[best_col]!=\"\"), df_screen[col])\n",
        "                df_screen.drop(columns=[best_col], inplace=True)\n",
        "        after_na = df_screen[\"sequence_accession\"].isna().sum() + (df_screen[\"sequence_accession\"] == \"\").sum()\n",
        "        merged_n = max(0, before_na - after_na)\n",
        "\n",
        "    # Write CSV\n",
        "    screen_csv = str(finish_out / \"enzyme_screening_sheet.csv\")\n",
        "    df_screen.to_csv(screen_csv, index=False)\n",
        "\n",
        "    # --- Markdown brief ---\n",
        "    md_path = str(finish_out / \"pathway_brief.md\")\n",
        "    lines = []\n",
        "    lines.append(f\"# Pathway {best_tag} — Enzyme Screening Summary\\n\")\n",
        "    lines.append(f\"**Total Steps:** {len(psteps)}\\n\")\n",
        "    lines.append(f\"**Ranking File:** `{Path(rank_csv).name}`\\n\")\n",
        "\n",
        "    def bullet(txt): return f\"- {txt}\"\n",
        "\n",
        "    for _, r in psteps.iterrows():\n",
        "        lines.append(f\"\\n## Step {int(r['step_idx'])}: {r.get('retrobiocat_reaction','')}\\n\")\n",
        "        lines.append(bullet(f\"Reaction SMILES: `{r.get('reaction_smiles','')}`\"))\n",
        "        lines.append(bullet(f\"Selected enzyme: {r.get('selected_enzyme','-')}\"))\n",
        "        lines.append(bullet(f\"Possible enzymes: {r.get('possible_enzymes','-')}\"))\n",
        "        if \"precedent_best_similarity\" in psteps.columns and pd.notna(r.get(\"precedent_best_similarity\")):\n",
        "            lines.append(bullet(f\"Precedent similarity: {r.get('precedent_best_similarity')}\"))\n",
        "        if r.get(\"selenzyme_url\"):\n",
        "            lines.append(bullet(f\"[Selenzyme link]({r.get('selenzyme_url')})\"))\n",
        "        # If we merged a top sequence for this step, surface it\n",
        "        if merged_n and \"sequence_accession\" in df_screen.columns:\n",
        "            acc = df_screen.loc[df_screen[\"step_idx\"]==int(r[\"step_idx\"]),\"sequence_accession\"].values[0]\n",
        "            org = df_screen.loc[df_screen[\"step_idx\"]==int(r[\"step_idx\"]),\"source_organism\"].values[0]\n",
        "            up  = df_screen.loc[df_screen[\"step_idx\"]==int(r[\"step_idx\"]),\"uniprot_url\"].values[0]\n",
        "            if acc:\n",
        "                lines.append(bullet(f\"Top sequence: [{acc}]({up}) ({org})\"))\n",
        "        else:\n",
        "            # fallback: include UniProt search for the selected enzyme name\n",
        "            if r.get(\"selected_enzyme\"):\n",
        "                q = quote_plus(r.get(\"selected_enzyme\"))\n",
        "                lines.append(bullet(f\"[UniProt search](https://www.uniprot.org/uniprotkb?query={q})\"))\n",
        "\n",
        "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "    # --- Update state/logs ---\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"screening_sheet_csv\": screen_csv,\n",
        "        \"pathway_brief_md\": md_path,\n",
        "        \"doe_pathway_tag\": best_tag,\n",
        "        \"doe_steps\": len(psteps),\n",
        "        \"doe_sequences_merged\": int(merged_n),\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"DoE sheet + brief ready for {best_tag}\",\n",
        "        f\"screening={screen_csv}\",\n",
        "        f\"brief={md_path}\",\n",
        "        f\"sequences merged into DoE: {merged_n}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd, json\n",
        "\n",
        "def human_gate_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Pause-and-review checkpoint node:\n",
        "      - dumps a compact summary of key artifacts\n",
        "      - waits for external 'approved' signal (LangGraph Interrupt equivalent)\n",
        "      - by default sets approved=False\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    summary = {\n",
        "        \"top_pathway\": state.get(\"doe_pathway_tag\"),\n",
        "        \"steps\": state.get(\"doe_steps\"),\n",
        "        \"thermo_method\": state.get(\"thermo_method\"),\n",
        "        \"sequence_rank_counts\": state.get(\"sequence_rank_counts\", {}),\n",
        "        \"screening_sheet\": state.get(\"screening_sheet_csv\"),\n",
        "        \"brief\": state.get(\"pathway_brief_md\"),\n",
        "    }\n",
        "\n",
        "    gate_path = finish_out / \"_HUMAN_GATE.md\"\n",
        "    with open(gate_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Human Approval Gate\\n\\n\")\n",
        "        f.write(\"Please review the current design-build-test-learn outputs:\\n\\n\")\n",
        "        for k, v in summary.items():\n",
        "            f.write(f\"- **{k}**: {v}\\n\")\n",
        "        f.write(\"\\nMark `approved=True` in the next LangGraph signal to continue.\\n\")\n",
        "\n",
        "    # detect external signal (if your LangGraph agents push state[\"signals\"])\n",
        "    signals = state.get(\"signals\", {})\n",
        "    approved = bool(signals.get(\"human_approved\")) or state.get(\"approved\", False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"approved\": approved,\n",
        "        \"human_gate_md\": str(gate_path),\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"human_gate: approval file ready at {gate_path}\",\n",
        "        f\"human_gate: approved={approved}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "import os, json, shutil, datetime\n",
        "from pathlib import Path\n",
        "\n",
        "def export_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Export all key DBTL artifacts (CSVs, MDs) and write a manifest.json.\n",
        "    Optionally could push to ELN/LIMS/GSheet later.\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    export_dir = workdir / \"retro_export\"\n",
        "    export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Collect all relevant artifacts from state\n",
        "    artifacts = []\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, str) and any(v.endswith(ext) for ext in [\".csv\",\".md\",\".json\",\".xlsx\"]):\n",
        "            if os.path.exists(v):\n",
        "                dst = export_dir / Path(v).name\n",
        "                try:\n",
        "                    shutil.copy2(v, dst)\n",
        "                    artifacts.append(str(dst))\n",
        "                except Exception as e:\n",
        "                    state.setdefault(\"logs\", []).append(f\"export_node: failed to copy {v}: {e}\")\n",
        "\n",
        "    # Write manifest\n",
        "    manifest = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"export_dir\": str(export_dir),\n",
        "        \"artifacts\": artifacts,\n",
        "        \"meta\": {\n",
        "            \"pathway_tag\": state.get(\"doe_pathway_tag\"),\n",
        "            \"approved\": state.get(\"approved\"),\n",
        "            \"thermo_method\": state.get(\"thermo_method\"),\n",
        "        },\n",
        "    }\n",
        "    manifest_path = export_dir / \"manifest.json\"\n",
        "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"export_manifest\": str(manifest_path),\n",
        "        \"export_dir\": str(export_dir),\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"export_node: exported {len(artifacts)} artifacts\",\n",
        "        f\"export_node: manifest={manifest_path}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "import time\n",
        "def wrap_node(name, fn):\n",
        "    def _wrapped(state):\n",
        "        t0 = time.time()\n",
        "        state = {**state, \"status\": f\"running:{name}\", \"last_node\": name,\n",
        "                 \"logs\": [*state.get(\"logs\", []), f\"▶ start {name}\"]}\n",
        "        out = fn(state)\n",
        "        dt = time.time() - t0\n",
        "        out_logs = [*out.get(\"logs\", []), f\"✔ done {name} ({dt:.1f}s)\"]\n",
        "        return {**out, \"status\": f\"idle:{name}\", \"logs\": out_logs, \"last_node\": name}\n",
        "    return RunnableLambda(_wrapped)\n",
        "\n",
        "def human_gate_node(state):\n",
        "    approved = bool(state.get(\"signals\", {}).get(\"human_approved\")) or state.get(\"approved\", False)\n",
        "    logs = [*state.get(\"logs\", [])]\n",
        "    logs.append(f\"human_gate: approved={approved}\")\n",
        "    return {**state, \"approved\": approved, \"logs\": logs}\n",
        "\n",
        "def build_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Build phase: fetch enzyme sequences for each selected enzyme in steps_enzyme_plan.csv.\n",
        "    - Queries UniProt/Selenzyme to fetch FASTA or metadata.\n",
        "    - Writes sequences_build_plan.csv and FASTA bundle.\n",
        "    \"\"\"\n",
        "    import os, pandas as pd, requests\n",
        "    from urllib.parse import quote_plus\n",
        "    from pathlib import Path\n",
        "\n",
        "    steps_csv = Path(state[\"steps_plan_csv\"])\n",
        "    out_dir   = Path(state[\"workdir\"]) / \"build_out\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    steps = pd.read_csv(steps_csv)\n",
        "    seq_rows = []\n",
        "    for _, r in steps.iterrows():\n",
        "        enzyme = str(r.get(\"selected_enzyme\", \"\")).strip()\n",
        "        if not enzyme or enzyme == \"nan\":\n",
        "            continue\n",
        "        q = quote_plus(enzyme)\n",
        "        uniprot_url = f\"https://rest.uniprot.org/uniprotkb/search?query={q}&format=tsv&fields=accession,organism,protein_name,length\"\n",
        "        try:\n",
        "            res = requests.get(uniprot_url, timeout=10)\n",
        "            if res.ok:\n",
        "                lines = res.text.splitlines()\n",
        "                if len(lines) > 1:\n",
        "                    acc, org, name, length = lines[1].split(\"\\t\")\n",
        "                    seq_rows.append({\n",
        "                        \"step_idx\": int(r[\"step_idx\"]),\n",
        "                        \"selected_enzyme\": enzyme,\n",
        "                        \"sequence_accession\": acc,\n",
        "                        \"source_organism\": org,\n",
        "                        \"uniprot_url\": f\"https://www.uniprot.org/uniprotkb/{acc}\",\n",
        "                        \"protein_name\": name,\n",
        "                        \"length\": length,\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            seq_rows.append({\"step_idx\": r[\"step_idx\"], \"selected_enzyme\": enzyme, \"error\": str(e)})\n",
        "\n",
        "    seq_df = pd.DataFrame(seq_rows)\n",
        "    seq_csv = out_dir / \"sequence_candidates.csv\"\n",
        "    seq_df.to_csv(seq_csv, index=False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"build_out_dir\": str(out_dir),\n",
        "        \"sequence_csv\": str(seq_csv),\n",
        "        **log(state, f\"build: found {len(seq_rows)} sequence candidates\")\n",
        "    }\n",
        "    return new_state\n",
        "\n",
        "\n",
        "graph = StateGraph(DBTLState)\n",
        "\n",
        "graph.add_node(\"retrosynthesis\", wrap_node(\"retrosynthesis\", retrosynthesis_node))\n",
        "graph.add_node(\"extract\",       wrap_node(\"extract\",       extract_pathways_node))\n",
        "graph.add_node(\"thermo\",        wrap_node(\"thermo\",        thermo_score_node))\n",
        "graph.add_node(\"rank\",          wrap_node(\"rank\",          rank_node))\n",
        "graph.add_node(\"selenzyme\",     wrap_node(\"selenzyme\",     selenzyme_node))\n",
        "graph.add_node(\"doe\",           wrap_node(\"doe\",           doe_node))\n",
        "graph.add_node(\"build\", RunnableLambda(build_node))\n",
        "graph.add_node(\"approve\",       wrap_node(\"approve\",       human_gate_node))\n",
        "graph.add_node(\"export\",        wrap_node(\"export\",        export_node))\n",
        "\n",
        "graph.set_entry_point(\"retrosynthesis\")\n",
        "graph.add_edge(\"retrosynthesis\",\"extract\")\n",
        "graph.add_edge(\"extract\",\"thermo\")\n",
        "graph.add_edge(\"thermo\",\"rank\")\n",
        "graph.add_edge(\"rank\",\"selenzyme\")\n",
        "graph.add_edge(\"selenzyme\",\"doe\")\n",
        "graph.add_edge(\"doe\", \"build\")\n",
        "graph.add_edge(\"build\", \"approve\")   # or -> simulate_node, if you prefer\n",
        "graph.add_edge(\"doe\",\"approve\")\n",
        "\n",
        "from langgraph.graph import END\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"approve\",\n",
        "    lambda s: \"export\" if s.get(\"approved\") else \"__end__\",   # ← return literal \"__end__\"\n",
        "    {\n",
        "        \"export\": \"export\",\n",
        "        \"__end__\": END,                                       # ← map \"__end__\" to END\n",
        "    },\n",
        ")\n",
        "\n",
        "graph.add_edge(\"export\", END)\n",
        "\n",
        "app = graph.compile()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "zwq_wYNnMdjU",
        "outputId": "15afe6a9-0672-4733-d5d6-a9b843d4ca4c"
      },
      "id": "zwq_wYNnMdjU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.8)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (0.3.77)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.2)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "init_state = {\n",
        "  \"run_id\": \"run_001\",\n",
        "  \"target_smiles\": \"COC1=C(C=CC(=C1)C=O)O\",\n",
        "  \"host\": \"Escherichia coli\",\n",
        "  \"constraints\": {\"max_steps\": \"5\"},\n",
        "  \"workdir\": f\"/content/runs/run_001\",\n",
        "  \"logs\": []\n",
        "}\n",
        "\n",
        "for state in app.stream(init_state, stream_mode=\"values\"):\n",
        "    node   = state.get(\"last_node\", \"?\")\n",
        "    status = state.get(\"status\", \"?\")\n",
        "    logs   = state.get(\"logs\", [])\n",
        "    last_log = logs[-1] if logs else \"\"     # ✅ safe guard\n",
        "    print(f\"[{node}] {status} | {last_log}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "cellView": "form",
        "id": "UaFCDh7yzeuF",
        "outputId": "6ebed47f-61aa-4455-8af9-92508d2d28c3"
      },
      "id": "UaFCDh7yzeuF",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[?] ? | \n",
            "[?] ? | ✔ done retrosynthesis (40.9s)\n",
            "[?] ? | ✔ done extract (0.0s)\n",
            "[?] ? | ✔ done thermo (0.6s)\n",
            "[?] ? | ✔ done rank (0.0s)\n",
            "[?] ? | ✔ done selenzyme (34.1s)\n",
            "[?] ? | ✔ done doe (0.0s)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "At key 'run_id': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-865027340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mnode\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_node\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                             \u001b[0mstream_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                         )\n\u001b[0;32m-> 2667\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m                     \u001b[0;31m# wait for checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdurability_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sync\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_loop.py\u001b[0m in \u001b[0;36mafter_tick\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mwrites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# all tasks have finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         self.updated_channels = apply_writes(\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_algo.py\u001b[0m in \u001b[0;36mapply_writes\u001b[0;34m(checkpoint, channels, tasks, get_next_version, trigger_to_nodes)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpending_writes_by_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnext_version\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"channel_versions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchan\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;31m# unavailable channels can't trigger tasks, so don't add them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/channels/last_value.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_CONCURRENT_GRAPH_UPDATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             )\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: At key 'run_id': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "New Version"
      ],
      "metadata": {
        "id": "Xki5swf5iUtJ"
      },
      "id": "Xki5swf5iUtJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed in a fresh notebook:\n",
        "!pip -q install langgraph langchain_core\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, sys, re, json, time, math, glob, shutil, textwrap, datetime, subprocess\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "import pandas as pd\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# =========================\n",
        "# State schema (fixes LastValue error by marking \"run_id\" static)\n",
        "# =========================\n",
        "class DBTLState(TypedDict, total=False):\n",
        "    # Static keys (won’t change across steps; avoids InvalidUpdateError)\n",
        "    run_id: Annotated[str, \"static\"]\n",
        "    workdir: Annotated[str, \"static\"]\n",
        "\n",
        "    # Inputs\n",
        "    target_smiles: str\n",
        "    host: str\n",
        "    constraints: Dict[str, Any]\n",
        "\n",
        "    # Bookkeeping\n",
        "    status: str\n",
        "    last_node: str\n",
        "    logs: List[str]\n",
        "    error: str\n",
        "    signals: Dict[str, Any]\n",
        "    approved: bool\n",
        "\n",
        "    # Retro out\n",
        "    retro_out_dir: str\n",
        "    pathways_all_csv: Optional[str]\n",
        "    pathways_solved_csv: Optional[str]\n",
        "    pathways_all_json: Optional[str]\n",
        "    pathways_solved_json: Optional[str]\n",
        "    pathway_json_map: Dict[str, str]\n",
        "\n",
        "    # Extractor / Thermo / Rank\n",
        "    steps_plan_csv: Optional[str]\n",
        "    steps_annotated_csv: Optional[str]\n",
        "    ranked_csv: Optional[str]\n",
        "    ranked_final_csv: Optional[str]\n",
        "    ranking_method: Optional[str]\n",
        "    thermo_method: Optional[str]\n",
        "\n",
        "    # Selenzyme + sequences\n",
        "    selenzyme_rows: int\n",
        "    scraped_sequences_csv: Optional[str]\n",
        "    sequences_ranked_csv: Optional[str]\n",
        "    sequences_shortlist_csv: Optional[str]\n",
        "    sequence_rank_method: Optional[str]\n",
        "    sequence_rank_counts: Dict[str, int]\n",
        "\n",
        "    # DoE\n",
        "    screening_sheet_csv: Optional[str]\n",
        "    pathway_brief_md: Optional[str]\n",
        "    doe_pathway_tag: Optional[str]\n",
        "    doe_steps: Optional[int]\n",
        "    doe_sequences_merged: Optional[int]\n",
        "\n",
        "    # Build\n",
        "    build_out_dir: Optional[str]\n",
        "    sequence_csv: Optional[str]\n",
        "\n",
        "    # Export\n",
        "    export_manifest: Optional[str]\n",
        "    export_dir: Optional[str]\n",
        "    human_gate_md: Optional[str]\n",
        "\n",
        "\n",
        "def log(state: DBTLState, msg: str) -> Dict[str, List[str]]:\n",
        "    return {\"logs\": (state.get(\"logs\", []) + [msg])}\n",
        "\n",
        "\n",
        "# ============== Helpers ==============\n",
        "def _run(cmd, timeout=1800, env=None, cwd=None, check=True) -> subprocess.CompletedProcess:\n",
        "    quiet = os.environ.get(\"DBTL_QUIET\") == \"1\"\n",
        "    # also let the graph control verbosity via constraints\n",
        "    try:\n",
        "        # best-effort: many nodes pass the same env; we check a global here too\n",
        "        quiet = quiet or globals().get(\"_DBTL_QUIET\", False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if not quiet:\n",
        "        print(f\"[run] {cmd}\")\n",
        "    res = subprocess.run(\n",
        "        cmd,\n",
        "        shell=True,\n",
        "        cwd=cwd,\n",
        "        env=env or os.environ.copy(),\n",
        "        stdout=(subprocess.DEVNULL if quiet else subprocess.PIPE),\n",
        "        stderr=(subprocess.STDOUT if not quiet else subprocess.DEVNULL),\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=check,\n",
        "    )\n",
        "    if not quiet and res.stdout:\n",
        "        print(res.stdout)\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "def wrap_node(name: str, fn):\n",
        "    \"\"\"Add start/finish logs + timing to any node.\"\"\"\n",
        "    def _wrapped(state: DBTLState) -> DBTLState:\n",
        "        t0 = time.time()\n",
        "        pre = {**state,\n",
        "               \"status\": f\"running:{name}\",\n",
        "               \"last_node\": name,\n",
        "               \"logs\": [*state.get(\"logs\", []), f\"▶ start {name}\"]}\n",
        "        out = fn(pre)\n",
        "        dt = time.time() - t0\n",
        "        out_logs = [*out.get(\"logs\", []), f\"✔ done {name} ({dt:.1f}s)\"]\n",
        "        return {**out, \"status\": f\"idle:{name}\", \"logs\": out_logs, \"last_node\": name}\n",
        "    return RunnableLambda(_wrapped)\n",
        "\n",
        "\n",
        "# ============== Nodes ==============\n",
        "\n",
        "def retrosynthesis_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    End-to-end RetroBioCat-2 retrosynthesis:\n",
        "      - micromamba bootstrap (local to workdir)\n",
        "      - create env + install RBC2 and pins\n",
        "      - run MCTS on target_smiles\n",
        "      - write all/solved pathways (CSV+JSON) + per-pathway JSONs\n",
        "    \"\"\"\n",
        "    import shutil as _sh\n",
        "    from pathlib import Path as _P\n",
        "\n",
        "    try:\n",
        "        workdir = _P(state[\"workdir\"]).expanduser().resolve()\n",
        "        target  = state.get(\"target_smiles\")\n",
        "        if not target:\n",
        "            raise ValueError(\"retrosynthesis_node: state['target_smiles'] is required\")\n",
        "\n",
        "        cons = (state.get(\"constraints\") or {})\n",
        "        max_time   = int(cons.get(\"max_search_time\", 15))\n",
        "        expanders  = list(cons.get(\"expanders\", [\"retrobiocat\"]))\n",
        "        max_depth  = cons.get(\"max_depth\", None)  # optional\n",
        "        force_rerun= bool(cons.get(\"rerun\", False))\n",
        "\n",
        "        run_dir   = workdir\n",
        "        retro_out = run_dir / \"retro_out\"\n",
        "        retro_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # micromamba layout inside run_dir\n",
        "        mm_root  = run_dir / \"micromamba\"\n",
        "        mm_bin   = run_dir / \"bin\" / \"micromamba\"\n",
        "        env_pref = mm_root / \"envs\" / \"retrobiocat\"\n",
        "\n",
        "        # bootstrap micromamba\n",
        "        if not mm_bin.exists():\n",
        "            _run(f\"curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest -o {run_dir}/mm.tar.bz2\")\n",
        "            _run(f\"tar -xvjf {run_dir}/mm.tar.bz2 -C {run_dir} bin/micromamba\")\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        env[\"MAMBA_ROOT_PREFIX\"] = str(mm_root)\n",
        "\n",
        "        # create env if missing\n",
        "        if force_rerun and env_pref.exists():\n",
        "            _sh.rmtree(env_pref, ignore_errors=True)\n",
        "        if not env_pref.exists():\n",
        "            _run(f\"{mm_bin} create -y -p {env_pref} -c conda-forge python=3.10 pip\", env=env)\n",
        "\n",
        "        # install RBC2 + pins\n",
        "        cmds = [\n",
        "            f\"{mm_bin} run -p {env_pref} python -m pip install --upgrade pip setuptools wheel\",\n",
        "            # try zip first, then fallback to git\n",
        "            f\"{mm_bin} run -p {env_pref} python -m pip install https://github.com/willfinnigan/RetroBioCat-2/archive/refs/heads/main.zip\"\n",
        "            f\" || {mm_bin} run -p {env_pref} python -m pip install 'git+https://github.com/willfinnigan/RetroBioCat-2.git'\",\n",
        "            f\"{mm_bin} run -p {env_pref} python -m pip install 'pydantic<2' 'networkx<3' tqdm requests\",\n",
        "        ]\n",
        "        for c in cmds:\n",
        "            _run(c, env=env)\n",
        "\n",
        "        # inline Python payload to run MCTS\n",
        "        py = textwrap.dedent(f\"\"\"\n",
        "            import json, pandas as pd\n",
        "            from pathlib import Path\n",
        "            from rbc2 import MCTS, get_expanders\n",
        "\n",
        "            retro_out = Path(r\"{retro_out}\")\n",
        "            retro_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            target_smi = r\"{target}\"\n",
        "            expanders  = get_expanders({json.dumps(expanders)})\n",
        "\n",
        "            mcts = MCTS(target_smi, expanders)\n",
        "            mcts.config.max_search_time = int({max_time})\n",
        "            {\"mcts.config.max_depth = int(%d)\" % int(max_depth) if max_depth is not None else \"\"}\n",
        "\n",
        "            print(\"[rbc2] Running MCTS …\")\n",
        "            mcts.run()\n",
        "\n",
        "            all_paths    = mcts.get_all_pathways()\n",
        "            solved_paths = mcts.get_solved_pathways()\n",
        "\n",
        "            def pathway_rows(pwy, tag):\n",
        "                rows = []\n",
        "                for i, rxn in enumerate(pwy.reactions, start=1):\n",
        "                    rows.append({{\n",
        "                        \"pathway_tag\": tag,\n",
        "                        \"step_idx\": i,\n",
        "                        \"reaction_smiles\": rxn.reaction_smiles(),\n",
        "                        \"product\": rxn.product,\n",
        "                        \"substrates\": \" . \".join(rxn.substrates),\n",
        "                        \"rxn_type\": rxn.rxn_type,\n",
        "                        \"name\": rxn.name,\n",
        "                        \"score\": rxn.score,\n",
        "                    }})\n",
        "                return rows\n",
        "\n",
        "            def write_bundle(paths, prefix):\n",
        "                all_rows = []\n",
        "                tag_to_file = {{}}\n",
        "                for k, pwy in enumerate(paths, start=1):\n",
        "                    tag = f\"P{{k:03d}}\"\n",
        "                    pjson = retro_out / f\"{{tag}}.json\"\n",
        "                    with open(pjson, \"w\", encoding=\"utf-8\") as f:\n",
        "                        json.dump(pwy.save(), f, indent=2)\n",
        "                    tag_to_file[tag] = str(pjson)\n",
        "                    all_rows.extend(pathway_rows(pwy, tag))\n",
        "                if all_rows:\n",
        "                    df = pd.DataFrame(all_rows)\n",
        "                    df.to_csv(retro_out / f\"pathways_{{prefix}}_steps.csv\", index=False)\n",
        "                    with open(retro_out / f\"pathways_{{prefix}}_steps.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                        json.dump(all_rows, f, indent=2)\n",
        "                return tag_to_file\n",
        "\n",
        "            tag_map_all = write_bundle(all_paths, \"all\")\n",
        "            tag_map_sol = write_bundle(solved_paths, \"solved\")\n",
        "\n",
        "            print(f\"[rbc2] Total pathways: {{len(all_paths)}} | Solved: {{len(solved_paths)}}\")\n",
        "            print(\"[rbc2] Outputs written to:\", retro_out)\n",
        "        \"\"\").strip()\n",
        "\n",
        "        _run(f\"\"\"{mm_bin} run -p {env_pref} python - <<'PY'\\n{py}\\nPY\"\"\", env=env)\n",
        "\n",
        "        # collect outputs\n",
        "        pathways_all_csv      = str(retro_out / \"pathways_all_steps.csv\")\n",
        "        pathways_solved_csv   = str(retro_out / \"pathways_solved_steps.csv\")\n",
        "        pathways_all_json     = str(retro_out / \"pathways_all_steps.json\")\n",
        "        pathways_solved_json  = str(retro_out / \"pathways_solved_steps.json\")\n",
        "        pjson_files = glob.glob(str(retro_out / \"P*.json\"))\n",
        "        tag_map = {Path(p).stem: str(Path(p)) for p in pjson_files}\n",
        "\n",
        "        new_state = {\n",
        "            **state,\n",
        "            \"retro_out_dir\": str(retro_out),\n",
        "            \"pathways_all_csv\": pathways_all_csv if os.path.exists(pathways_all_csv) else None,\n",
        "            \"pathways_solved_csv\": pathways_solved_csv if os.path.exists(pathways_solved_csv) else None,\n",
        "            \"pathways_all_json\": pathways_all_json if os.path.exists(pathways_all_json) else None,\n",
        "            \"pathways_solved_json\": pathways_solved_json if os.path.exists(pathways_solved_json) else None,\n",
        "            \"pathway_json_map\": tag_map,\n",
        "        }\n",
        "        new_state[\"logs\"] = [*new_state.get(\"logs\", []), \"retrosynthesis done\"]\n",
        "        return new_state\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        err = f\"retrosynthesis_node: subprocess failed (code={e.returncode})\"\n",
        "        new_state = {**state, \"error\": err}\n",
        "        new_state[\"logs\"] = [*new_state.get(\"logs\", []), err, str(e)]\n",
        "        return new_state\n",
        "    except Exception as e:\n",
        "        err = f\"retrosynthesis_node: {type(e).__name__}: {e}\"\n",
        "        new_state = {**state, \"error\": err}\n",
        "        new_state[\"logs\"] = [*new_state.get(\"logs\", []), err]\n",
        "        return new_state\n",
        "\n",
        "\n",
        "def extract_pathways_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Build enzyme-planning artifacts from RetroBioCat-2 outputs:\n",
        "      - steps_enzyme_plan.csv (per-step details incl. enzyme classes, precedents, Selenzyme links)\n",
        "      - pathways_ranked_no_thermo.csv (RBC2 + precedent-based ranking; no thermodynamics)\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    retro_out = workdir / \"retro_out\"\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # pick CSV (solved preferred, else all)\n",
        "    csv_solved = retro_out / \"pathways_solved_steps.csv\"\n",
        "    csv_all    = retro_out / \"pathways_all_steps.csv\"\n",
        "    csv_in = csv_solved if csv_solved.exists() else csv_all\n",
        "    if not csv_in.exists():\n",
        "        raise FileNotFoundError(f\"No pathways CSV found under {retro_out}\")\n",
        "\n",
        "    steps_df = pd.read_csv(csv_in)\n",
        "\n",
        "    # pathway -> json map\n",
        "    json_map: Dict[str,str] = dict(state.get(\"pathway_json_map\") or {})\n",
        "    if not json_map:\n",
        "        for p in glob.glob(str(retro_out / \"P*.json\")):\n",
        "            tag = Path(p).stem  # P001\n",
        "            json_map[tag] = p\n",
        "\n",
        "    from urllib.parse import quote_plus\n",
        "\n",
        "    def selenzyme_url_from_rxn(rxn_smiles: str, org: str) -> str:\n",
        "        return (\n",
        "            \"https://selenzyme.synbiochem.co.uk/selenzy/selenzy?reaction_smiles=\"\n",
        "            + quote_plus(rxn_smiles)\n",
        "            + \"&organism=\"\n",
        "            + quote_plus(org)\n",
        "        )\n",
        "\n",
        "    host_org = state.get(\"host\") or \"Escherichia coli\"\n",
        "\n",
        "    # expand per-step details from Pxxx.json\n",
        "    records = []\n",
        "    for _, r in steps_df.iterrows():\n",
        "        tag   = str(r.get(\"pathway_tag\"))\n",
        "        idx   = int(r.get(\"step_idx\"))\n",
        "        rxn   = str(r.get(\"reaction_smiles\",\"\"))\n",
        "        prod  = str(r.get(\"products\", r.get(\"product\",\"\")))\n",
        "        subs  = str(r.get(\"substrates\",\"\"))\n",
        "\n",
        "        rxn_type = r.get(\"rxn_type\",\"\")\n",
        "        name     = r.get(\"name\",\"\")\n",
        "        rbc2_score = r.get(\"score\", float(\"nan\"))\n",
        "\n",
        "        possible_enzymes = []\n",
        "        enzyme_choices = []\n",
        "        selected_enzyme = \"\"\n",
        "        precedents = []\n",
        "\n",
        "        pj = json_map.get(tag)\n",
        "        if pj and os.path.exists(pj):\n",
        "            try:\n",
        "                data = json.load(open(pj, \"r\"))\n",
        "                if isinstance(data, list) and 1 <= idx <= len(data):\n",
        "                    d = data[idx - 1]\n",
        "                    rxn_type = d.get(\"rxn_type\", rxn_type)\n",
        "                    name     = d.get(\"name\", name)\n",
        "                    tm = d.get(\"template_metadata\", {}) or {}\n",
        "                    if tm:\n",
        "                        tkey = next(iter(tm.keys()))\n",
        "                        tmeta = tm.get(tkey, {}) or {}\n",
        "                        possible_enzymes = (tmeta.get(\"possible_enzymes\") or [])\n",
        "                        choices = (tmeta.get(\"enzyme_choices\") or [])\n",
        "                        enzyme_choices = [\n",
        "                            \" / \".join(x) if isinstance(x, (list, tuple)) else str(x)\n",
        "                            for x in choices\n",
        "                        ]\n",
        "                        selected_enzyme = tmeta.get(\"selected_enzyme\") or \"\"\n",
        "                    precedents = d.get(\"precedents\") or []\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        prec_sorted = sorted(precedents, key=lambda x: x.get(\"similarity\", 0), reverse=True)[:3]\n",
        "        prec_summ, best_sim = [], float(\"nan\")\n",
        "        for p in prec_sorted:\n",
        "            sim = p.get(\"similarity\", float(\"nan\"))\n",
        "            dat = p.get(\"data\", {}) or {}\n",
        "            doi = dat.get(\"html_doi\", \"\") or dat.get(\"doi\", \"\")\n",
        "            enz = dat.get(\"enzyme_name\", \"\") or p.get(\"name\", \"\")\n",
        "            cite = dat.get(\"short_citation\", \"\")\n",
        "            try:\n",
        "                prec_summ.append(f\"{enz} | sim={float(sim):.3f} | {cite} | {doi}\")\n",
        "            except Exception:\n",
        "                prec_summ.append(f\"{enz} | sim={sim} | {cite} | {doi}\")\n",
        "        if prec_sorted:\n",
        "            best_sim = prec_sorted[0].get(\"similarity\", float(\"nan\"))\n",
        "\n",
        "        records.append({\n",
        "            \"pathway_tag\": tag,\n",
        "            \"step_idx\": idx,\n",
        "            \"rxn_type\": rxn_type,\n",
        "            \"retrobiocat_reaction\": name,\n",
        "            \"reaction_smiles\": rxn,\n",
        "            \"substrates\": subs,\n",
        "            \"products\": prod,\n",
        "            \"rbc2_score\": rbc2_score,\n",
        "            \"selected_enzyme\": selected_enzyme,\n",
        "            \"possible_enzymes\": \" ; \".join(possible_enzymes),\n",
        "            \"enzyme_choices\": \" ; \".join(enzyme_choices),\n",
        "            \"precedent_top3\": \" || \".join(prec_summ),\n",
        "            \"precedent_best_similarity\": best_sim,\n",
        "            \"selenzyme_url\": selenzyme_url_from_rxn(rxn, host_org),\n",
        "        })\n",
        "\n",
        "    plan = pd.DataFrame(records).sort_values([\"pathway_tag\",\"step_idx\"]).reset_index(drop=True)\n",
        "\n",
        "    # pathway ranking (no thermo)\n",
        "    tmp = plan.copy()\n",
        "    tmp[\"rbc2_score\"] = pd.to_numeric(tmp[\"rbc2_score\"], errors=\"coerce\").fillna(0.0)\n",
        "    tmp[\"precedent_best_similarity\"] = pd.to_numeric(tmp[\"precedent_best_similarity\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    agg = tmp.groupby(\"pathway_tag\").agg(\n",
        "        steps_count=(\"step_idx\",\"max\"),\n",
        "        sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "        sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        "    ).reset_index()\n",
        "    agg[\"rank_score\"] = agg[\"sum_rbc2\"] + 0.2*agg[\"sum_prec_sim\"] - 0.1*agg[\"steps_count\"]\n",
        "    ranked = agg.sort_values(\n",
        "        [\"rank_score\",\"sum_rbc2\",\"sum_prec_sim\",\"steps_count\"],\n",
        "        ascending=[False, False, False, True]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    steps_plan_csv = str(finish_out / \"steps_enzyme_plan.csv\")\n",
        "    ranked_csv     = str(finish_out / \"pathways_ranked_no_thermo.csv\")\n",
        "    plan.to_csv(steps_plan_csv, index=False)\n",
        "    ranked.to_csv(ranked_csv, index=False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"steps_plan_csv\": steps_plan_csv,\n",
        "        \"ranked_csv\": ranked_csv,\n",
        "        \"thermo_method\": \"none\",\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"extracted pathways ({len(plan)} step rows, {len(ranked)} pathways)\",\n",
        "        f\"steps_plan={steps_plan_csv}\",\n",
        "        f\"ranked_no_thermo={ranked_csv}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def thermo_score_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Lightweight 'match-only' thermo pass (no ChemAxon / ΔG):\n",
        "      - Emits steps_annotated.csv and pathways_ranked.csv (same heuristic as extractor).\n",
        "    \"\"\"\n",
        "    workdir    = Path(state[\"workdir\"])\n",
        "    mm_root    = workdir / \"micromamba\"\n",
        "    env_prefix = mm_root / \"envs\" / \"retrobiocat\"  # same env\n",
        "    retro_out  = workdir / \"retro_out\"\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    steps_plan_csv = state.get(\"steps_plan_csv\")\n",
        "    if not steps_plan_csv or not Path(steps_plan_csv).exists():\n",
        "        csv_solved = retro_out / \"pathways_solved_steps.csv\"\n",
        "        csv_all    = retro_out / \"pathways_all_steps.csv\"\n",
        "        steps_plan_csv = str(csv_solved if csv_solved.exists() else csv_all)\n",
        "\n",
        "    steps_out = str(finish_out / \"steps_annotated.csv\")\n",
        "    rank_out  = str(finish_out / \"pathways_ranked.csv\")\n",
        "\n",
        "    py = f\"\"\"\\\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "IN  = Path(r\"{steps_plan_csv}\")\n",
        "OUT_STEPS = Path(r\"{steps_out}\")\n",
        "OUT_RANK  = Path(r\"{rank_out}\")\n",
        "\n",
        "if not IN.exists():\n",
        "    raise FileNotFoundError(f\"Input steps table not found: {{IN}}\")\n",
        "\n",
        "df = pd.read_csv(IN)\n",
        "\n",
        "# Ensure required columns exist\n",
        "for col in [\"pathway_tag\",\"step_idx\",\"reaction_smiles\",\"substrates\",\"products\",\n",
        "            \"rbc2_score\",\"precedent_best_similarity\",\"selenzyme_url\"]:\n",
        "    if col not in df.columns:\n",
        "        df[col] = \"\" if col not in [\"rbc2_score\",\"precedent_best_similarity\"] else 0.0\n",
        "\n",
        "# Add placeholders for thermo\n",
        "df[\"dGprime_kJ_per_mol\"] = float(\"nan\")\n",
        "df[\"uncert_kJ_per_mol\"]  = float(\"nan\")\n",
        "df[\"thermo_pass\"]        = False\n",
        "df[\"equilibrator_formula\"] = \"\"\n",
        "\n",
        "# Save annotated\n",
        "df.to_csv(OUT_STEPS, index=False)\n",
        "\n",
        "# Ranking (same as extractor)\n",
        "tmp = df.copy()\n",
        "tmp[\"rbc2_score\"] = pd.to_numeric(tmp[\"rbc2_score\"], errors=\"coerce\").fillna(0.0)\n",
        "tmp[\"precedent_best_similarity\"] = pd.to_numeric(tmp[\"precedent_best_similarity\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "agg = tmp.groupby(\"pathway_tag\").agg(\n",
        "    steps_count=(\"step_idx\",\"max\"),\n",
        "    sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "    sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        ").reset_index()\n",
        "\n",
        "agg[\"rank_score\"] = agg[\"sum_rbc2\"] + 0.2*agg[\"sum_prec_sim\"] - 0.1*agg[\"steps_count\"]\n",
        "ranked = agg.sort_values([\"rank_score\",\"sum_rbc2\",\"sum_prec_sim\",\"steps_count\"],\n",
        "                         ascending=[False,False,False,True]).reset_index(drop=True)\n",
        "ranked.to_csv(OUT_RANK, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" \", OUT_STEPS)\n",
        "print(\" \", OUT_RANK)\n",
        "\"\"\"\n",
        "    micromamba = workdir / \"bin\" / \"micromamba\"\n",
        "    if not micromamba.exists():\n",
        "        micromamba = \"micromamba\"\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"MAMBA_ROOT_PREFIX\"] = str(mm_root)\n",
        "\n",
        "    try:\n",
        "        _run(f\"\"\"{micromamba} run -p \"{env_prefix}\" python - <<'PY'\\n{py}\\nPY\"\"\", env=env)\n",
        "    except subprocess.CalledProcessError:\n",
        "        _run(f\"python - <<'PY'\\n{py}\\nPY\")\n",
        "\n",
        "    coverage = 0\n",
        "    method = \"matchonly\"\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"thermo_method\": method,\n",
        "        \"steps_annotated_csv\": steps_out,\n",
        "        \"ranked_csv\": rank_out,\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"thermo: {method}, coverage={coverage}\",\n",
        "        f\"steps_annotated={steps_out}\",\n",
        "        f\"ranked={rank_out}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def rank_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Merge thermo info (if available) into pathway ranking; else keep RBC2+precedent ranking.\n",
        "    Emits pathways_ranked_final.csv\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    steps_csv = state.get(\"steps_annotated_csv\") or state.get(\"steps_plan_csv\")\n",
        "    if not steps_csv or not Path(steps_csv).exists():\n",
        "        raise FileNotFoundError(\"rank_node: steps CSV not found in state\")\n",
        "\n",
        "    df = pd.read_csv(steps_csv)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    has_thermo = any(c in df.columns for c in [\"dGprime_kJ_per_mol\", \"thermo_pass\"])\n",
        "    thermo_mode = bool(has_thermo and df[\"thermo_pass\"].any())  # likely False in match-only\n",
        "\n",
        "    df[\"rbc2_score\"] = pd.to_numeric(df.get(\"rbc2_score\", 0.0), errors=\"coerce\").fillna(0.0)\n",
        "    df[\"precedent_best_similarity\"] = pd.to_numeric(df.get(\"precedent_best_similarity\", 0.0),\n",
        "                                                    errors=\"coerce\").fillna(0.0)\n",
        "    if \"dGprime_kJ_per_mol\" in df:\n",
        "        df[\"dGprime_kJ_per_mol\"] = pd.to_numeric(df[\"dGprime_kJ_per_mol\"], errors=\"coerce\")\n",
        "\n",
        "    group = df.groupby(\"pathway_tag\", dropna=True)\n",
        "\n",
        "    if thermo_mode:\n",
        "        ranked = (\n",
        "            group.agg(\n",
        "                steps_count=(\"step_idx\",\"max\"),\n",
        "                sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "                sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        "                thermo_pass_steps=(\"thermo_pass\",\"sum\"),\n",
        "                sum_dGprime=(\"dGprime_kJ_per_mol\",\"sum\"),\n",
        "            )\n",
        "            .reset_index()\n",
        "        )\n",
        "        ranked[\"all_steps_pass\"] = ranked[\"thermo_pass_steps\"] == ranked[\"steps_count\"]\n",
        "        ranked[\"rank_score\"] = (\n",
        "            ranked[\"sum_rbc2\"]\n",
        "            + 0.2 * ranked[\"sum_prec_sim\"]\n",
        "            - 0.1 * ranked[\"steps_count\"]\n",
        "            - 0.001 * ranked[\"sum_dGprime\"].fillna(0.0)\n",
        "            + ranked[\"all_steps_pass\"].astype(float) * 0.5\n",
        "        )\n",
        "        method = \"rbc2+precedent+thermo\"\n",
        "    else:\n",
        "        ranked = (\n",
        "            group.agg(\n",
        "                steps_count=(\"step_idx\",\"max\"),\n",
        "                sum_rbc2=(\"rbc2_score\",\"sum\"),\n",
        "                sum_prec_sim=(\"precedent_best_similarity\",\"sum\"),\n",
        "            )\n",
        "            .reset_index()\n",
        "        )\n",
        "        ranked[\"rank_score\"] = (\n",
        "            ranked[\"sum_rbc2\"] + 0.2 * ranked[\"sum_prec_sim\"] - 0.1 * ranked[\"steps_count\"]\n",
        "        )\n",
        "        method = \"rbc2+precedent\"\n",
        "\n",
        "    ranked = ranked.sort_values(\n",
        "        [\"rank_score\",\"sum_rbc2\",\"sum_prec_sim\",\"steps_count\"],\n",
        "        ascending=[False, False, False, True],\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    out_csv = str(finish_out / \"pathways_ranked_final.csv\")\n",
        "    ranked.to_csv(out_csv, index=False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"ranked_final_csv\": out_csv,\n",
        "        \"ranking_method\": method,\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"ranked pathways ({method}, {len(ranked)} entries)\",\n",
        "        f\"ranked_final={out_csv}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def selenzyme_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Collect Selenzyme links and try to scrape top sequences (polite, tolerant).\n",
        "    Falls back to counting links if blocked. Emits selenzyme_scrape.csv if rows found.\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    steps_csv = state.get(\"steps_annotated_csv\") or state.get(\"steps_plan_csv\")\n",
        "    if not steps_csv or not Path(steps_csv).exists():\n",
        "        raise FileNotFoundError(\"selenzyme_node: no steps CSV found in state\")\n",
        "\n",
        "    steps = pd.read_csv(steps_csv)\n",
        "    if \"selenzyme_url\" not in steps.columns:\n",
        "        new_state = {\n",
        "            **state,\n",
        "            \"selenzyme_rows\": 0,\n",
        "            \"scraped_sequences_csv\": None,\n",
        "        }\n",
        "        new_state[\"logs\"] = state.get(\"logs\", []) + [\"selenzyme: no 'selenzyme_url' column; skipped\"]\n",
        "        return new_state\n",
        "\n",
        "    url_map: Dict[str, List[Tuple[str,int]]] = {}\n",
        "    for _, r in steps.iterrows():\n",
        "        url = str(r.get(\"selenzyme_url\") or \"\").strip()\n",
        "        if not url:\n",
        "            continue\n",
        "        tag = str(r.get(\"pathway_tag\"))\n",
        "        idx = int(r.get(\"step_idx\", 0))\n",
        "        url_map.setdefault(url, []).append((tag, idx))\n",
        "\n",
        "    unique_urls = list(url_map.keys())\n",
        "\n",
        "    cfg = (state.get(\"constraints\", {}) or {}).get(\"selenzyme\", {}) or {}\n",
        "    do_scrape: bool   = bool(cfg.get(\"scrape\", True))\n",
        "    max_urls: int     = int(cfg.get(\"max_urls\", 40))\n",
        "    timeout_s: int    = int(cfg.get(\"timeout_s\", 15))\n",
        "    sleep_s: float    = float(cfg.get(\"sleep_s\", 1.0))\n",
        "\n",
        "    urls_to_hit = unique_urls[:max_urls]\n",
        "    scraped_rows: List[Dict] = []\n",
        "    blocked_reason: Optional[str] = None\n",
        "\n",
        "    def _try_scrape(url: str) -> List[Dict]:\n",
        "        nonlocal blocked_reason\n",
        "        try:\n",
        "            import requests\n",
        "            from bs4 import BeautifulSoup  # type: ignore\n",
        "\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36\",\n",
        "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "                \"Connection\": \"keep-alive\",\n",
        "            }\n",
        "            resp = requests.get(url, headers=headers, timeout=timeout_s)\n",
        "            if resp.status_code in (403, 429):\n",
        "                raise RuntimeError(f\"HTTP {resp.status_code}\")\n",
        "            if not resp.ok or not resp.text:\n",
        "                return []\n",
        "            html = resp.text\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            table = None\n",
        "            for t in soup.find_all(\"table\"):\n",
        "                ths = [th.get_text(strip=True).lower() for th in t.find_all(\"th\")]\n",
        "                joined = \" \".join(ths)\n",
        "                if any(k in joined for k in [\"uniprot\", \"organism\", \"score\", \"ec\", \"enzyme\"]):\n",
        "                    table = t\n",
        "                    break\n",
        "\n",
        "            rows_local: List[Dict] = []\n",
        "            if table:\n",
        "                headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
        "                idx_map = {h.lower(): i for i, h in enumerate(headers)}\n",
        "\n",
        "                def _get(td_list, key_variants):\n",
        "                    for k in key_variants:\n",
        "                        i = idx_map.get(k)\n",
        "                        if i is not None and i < len(td_list):\n",
        "                            return td_list[i].get_text(strip=True)\n",
        "                    return \"\"\n",
        "\n",
        "                for tr in table.find_all(\"tr\"):\n",
        "                    tds = tr.find_all(\"td\")\n",
        "                    if not tds:\n",
        "                        continue\n",
        "                    acc = _get(tds, [\"uniprot\", \"accession\", \"uniprot id\"])\n",
        "                    org = _get(tds, [\"organism\", \"source\", \"species\"])\n",
        "                    scr = _get(tds, [\"score\", \"similarity\", \"sim\"])\n",
        "                    ec  = _get(tds, [\"ec\", \"ec number\"])\n",
        "                    enz = _get(tds, [\"enzyme\", \"name\", \"protein\"])\n",
        "\n",
        "                    if not acc:\n",
        "                        a = tds[0].find(\"a\")\n",
        "                        if a and a.get_text(strip=True):\n",
        "                            acc = a.get_text(strip=True)\n",
        "                    if not acc:\n",
        "                        m = re.search(r\"[A-NR-Z0-9]{{6,10}}\", tr.get_text(\" \", strip=True))\n",
        "                        if m:\n",
        "                            acc = m.group(0)\n",
        "\n",
        "                    try:\n",
        "                        scr_val = float(re.sub(\"[^0-9.+-eE]\", \"\", scr)) if scr else None\n",
        "                    except Exception:\n",
        "                        scr_val = None\n",
        "\n",
        "                    rows_local.append({\n",
        "                        \"accession\": acc,\n",
        "                        \"organism\": org,\n",
        "                        \"score\": scr_val if scr_val is not None else scr,\n",
        "                        \"ec\": ec,\n",
        "                        \"enzyme_name\": enz,\n",
        "                    })\n",
        "\n",
        "            if not rows_local:\n",
        "                tokens = re.findall(r\"\\b[OPQ][0-9][A-Z0-9]{{3}}[0-9]\\b|\\b[A-NR-Z0-9]{{6}}\\b\", html)\n",
        "                rows_local = [{\"accession\": t, \"organism\": \"\", \"score\": \"\", \"ec\": \"\", \"enzyme_name\": \"\"} for t in dict.fromkeys(tokens)]\n",
        "\n",
        "            return rows_local\n",
        "\n",
        "        except Exception as e:\n",
        "            blocked_reason = str(e)\n",
        "            return []\n",
        "\n",
        "    if do_scrape and len(urls_to_hit) > 0:\n",
        "        for i, url in enumerate(urls_to_hit, 1):\n",
        "            rows_local = _try_scrape(url)\n",
        "            for (tag, idx) in url_map.get(url, []):\n",
        "                for r in rows_local:\n",
        "                    scraped_rows.append({\n",
        "                        \"pathway_tag\": tag,\n",
        "                        \"step_idx\": idx,\n",
        "                        \"selenzyme_url\": url,\n",
        "                        **r\n",
        "                    })\n",
        "            if i < len(urls_to_hit):\n",
        "                try:\n",
        "                    time.sleep(sleep_s)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    scraped_csv_path: Optional[str] = None\n",
        "    if scraped_rows:\n",
        "        out_csv = finish_out / \"selenzyme_scrape.csv\"\n",
        "        pd.DataFrame(scraped_rows, columns=[\n",
        "            \"pathway_tag\",\"step_idx\",\"selenzyme_url\",\n",
        "            \"accession\",\"organism\",\"score\",\"ec\",\"enzyme_name\"\n",
        "        ]).to_csv(out_csv, index=False)\n",
        "        scraped_csv_path = str(out_csv)\n",
        "\n",
        "    total_links = len(unique_urls)\n",
        "    rows = len(scraped_rows)\n",
        "\n",
        "    log_lines = [\n",
        "        f\"selenzyme: links={total_links}, scraped_rows={rows}, mode={'scrape' if do_scrape else 'count-only'}\"\n",
        "    ]\n",
        "    if blocked_reason and rows == 0 and do_scrape:\n",
        "        log_lines.append(f\"selenzyme: scrape blocked or failed ({blocked_reason}); fell back to count-only\")\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"selenzyme_rows\": rows,\n",
        "        \"scraped_sequences_csv\": scraped_csv_path,\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + log_lines\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def sequence_rank_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Rank Selenzyme-scraped sequences per step and emit:\n",
        "      - sequences_ranked_by_step.csv\n",
        "      - sequences_shortlist.csv\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    scraped_csv = state.get(\"scraped_sequences_csv\")\n",
        "    if not scraped_csv or not Path(scraped_csv).exists():\n",
        "        new_state = { **state }\n",
        "        new_state[\"logs\"] = state.get(\"logs\", []) + [\"sequence_rank: no scraped_sequences_csv; skipped\"]\n",
        "        return new_state\n",
        "\n",
        "    steps_csv = state.get(\"steps_plan_csv\")\n",
        "    steps_order = None\n",
        "    if steps_csv and Path(steps_csv).exists():\n",
        "        try:\n",
        "            _steps = pd.read_csv(steps_csv)\n",
        "            steps_order = (\n",
        "                _steps[[\"pathway_tag\",\"step_idx\"]]\n",
        "                .drop_duplicates()\n",
        "                .sort_values([\"pathway_tag\",\"step_idx\"])\n",
        "            )\n",
        "        except Exception:\n",
        "            steps_order = None\n",
        "\n",
        "    cfg: Dict = (state.get(\"constraints\", {}) or {}).get(\"sequence_rank\", {}) or {}\n",
        "    TOP_N = int(cfg.get(\"top_n_per_step\", 5))\n",
        "    preferred_hosts = set(cfg.get(\"preferred_hosts\", []))\n",
        "    extra_bact = set(cfg.get(\"extra_bacterial_genera\", [\"Pseudomonas\",\"Corynebacterium\",\"Rhodococcus\",\"Acinetobacter\"]))\n",
        "    blacklist_re = re.compile(cfg.get(\"blacklist_regex\", r\"(mitochondrial|chloroplast)\"), flags=re.I)\n",
        "\n",
        "    df = pd.read_csv(scraped_csv)\n",
        "    for col in [\"pathway_tag\",\"step_idx\",\"selenzyme_url\",\"accession\",\"organism\",\"score\",\"ec\",\"enzyme_name\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = \"\" if col not in [\"score\",\"step_idx\"] else (0.0 if col==\"score\" else 0)\n",
        "\n",
        "    df[\"step_idx\"] = pd.to_numeric(df[\"step_idx\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    def _to_float(x):\n",
        "        try:\n",
        "            if pd.isna(x): return math.nan\n",
        "            return float(str(x).strip())\n",
        "        except Exception:\n",
        "            return math.nan\n",
        "    df[\"score_num\"] = df[\"score\"].apply(_to_float)\n",
        "\n",
        "    bacterial_keywords = [\n",
        "        \"bacter\", \"bacillus\",\"escherichia\",\"pseudomonas\",\"corynebacterium\",\"rhodococcus\",\n",
        "        \"acinetobacter\",\"enterobacter\",\"salmonella\",\"shigella\",\"staphylococcus\",\"streptococcus\",\n",
        "        \"lactobacillus\",\"lactococcus\",\"klebsiella\",\"burkholderia\",\"clostridium\",\"vibrio\",\n",
        "        \"shewanella\",\"xanthomonas\",\"mycobacterium\",\"yersinia\",\"pasteurella\",\"azotobacter\",\n",
        "        \"cupriavidus\",\"caulobacter\",\"alcaligenes\",\"sphingomonas\"\n",
        "    ] + [g.lower() for g in extra_bact]\n",
        "    euk_exclude = [\n",
        "        \"homo\",\"mus\",\"rattus\",\"danio\",\"drosophila\",\"bos\",\"gallus\",\"arabidopsis\",\"oryza\",\"zea\",\n",
        "        \"nicotiana\",\"saccharomyces\",\"pichia\",\"kluyveromyces\",\"aspergillus\",\"neurospora\",\"candida\"\n",
        "    ]\n",
        "    def is_bacterial(org: str) -> bool:\n",
        "        org_l = (org or \"\").lower()\n",
        "        if any(k in org_l for k in euk_exclude):\n",
        "            return False\n",
        "        if any(k in org_l for k in bacterial_keywords):\n",
        "            return True\n",
        "        return False\n",
        "    df[\"is_bacterial\"] = df[\"organism\"].apply(is_bacterial)\n",
        "\n",
        "    def in_preferred(org: str) -> bool:\n",
        "        if not preferred_hosts: return False\n",
        "        for ph in preferred_hosts:\n",
        "            if ph.lower() in (org or \"\").lower():\n",
        "                return True\n",
        "        return False\n",
        "    df[\"is_preferred_host\"] = df[\"organism\"].apply(in_preferred)\n",
        "    df[\"is_blacklisted\"] = df[\"organism\"].apply(lambda x: bool(blacklist_re.search(str(x))) if str(x) else False)\n",
        "\n",
        "    def acc_quality(acc: str) -> int:\n",
        "        s = (acc or \"\").strip()\n",
        "        return 1 if re.fullmatch(r\"[A-NR-Z0-9]{6,10}\", s) else 0\n",
        "    df[\"acc_quality\"] = df[\"accession\"].apply(acc_quality)\n",
        "\n",
        "    s = df[\"score_num\"]\n",
        "    if s.notna().sum() > 0:\n",
        "        def per_step_norm(g):\n",
        "            vals = g[\"score_num\"]\n",
        "            vmin = vals.min(skipna=True)\n",
        "            vmax = vals.max(skipna=True)\n",
        "            if pd.isna(vmin) or pd.isna(vmax) or vmin == vmax:\n",
        "                return pd.Series([0.5]*len(g), index=g.index)\n",
        "            return (vals - vmin) / (vmax - vmin)\n",
        "        df[\"score_norm\"] = df.groupby([\"pathway_tag\",\"step_idx\"], dropna=False, as_index=False).apply(per_step_norm).reset_index(level=[0,1], drop=True)\n",
        "        df[\"score_norm\"] = df[\"score_norm\"].fillna(0.5)\n",
        "    else:\n",
        "        df[\"score_norm\"] = 0.5\n",
        "\n",
        "    df[\"bonus_bacterial\"] = df[\"is_bacterial\"].astype(float) * 0.20\n",
        "    df[\"bonus_preferred\"] = df[\"is_preferred_host\"].astype(float) * 0.10\n",
        "    df[\"bonus_acc\"]       = df[\"acc_quality\"].astype(float) * 0.05\n",
        "    df[\"pen_blacklist\"]   = df[\"is_blacklisted\"].astype(float) * 0.30\n",
        "\n",
        "    df[\"rank_score\"] = df[\"score_norm\"] + df[\"bonus_bacterial\"] + df[\"bonus_preferred\"] + df[\"bonus_acc\"] - df[\"pen_blacklist\"]\n",
        "\n",
        "    df = df.sort_values([\"accession\",\"rank_score\"], ascending=[True, False])\n",
        "    df_dedup = df.drop_duplicates(subset=[\"accession\"], keep=\"first\")\n",
        "\n",
        "    top_by_step = (\n",
        "        df_dedup\n",
        "        .sort_values([\"pathway_tag\",\"step_idx\",\"rank_score\"], ascending=[True, True, False])\n",
        "        .groupby([\"pathway_tag\",\"step_idx\"], dropna=False)\n",
        "        .head(int(TOP_N))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    def uniprot_url(acc: str) -> str:\n",
        "        acc = str(acc).strip()\n",
        "        return f\"https://www.uniprot.org/uniprotkb/{acc}\" if acc else \"\"\n",
        "    top_by_step[\"uniprot_url\"] = top_by_step[\"accession\"].apply(uniprot_url)\n",
        "\n",
        "    cols = [\n",
        "        \"pathway_tag\",\"step_idx\",\"accession\",\"organism\",\"ec\",\"enzyme_name\",\n",
        "        \"score\",\"score_num\",\"score_norm\",\n",
        "        \"is_bacterial\",\"is_preferred_host\",\"is_blacklisted\",\"acc_quality\",\n",
        "        \"bonus_bacterial\",\"bonus_preferred\",\"bonus_acc\",\"pen_blacklist\",\n",
        "        \"rank_score\",\"selenzyme_url\",\"uniprot_url\"\n",
        "    ]\n",
        "    cols = [c for c in cols if c in top_by_step.columns]\n",
        "    top_by_step = top_by_step[cols]\n",
        "\n",
        "    shortlist = (\n",
        "        top_by_step\n",
        "        .sort_values([\"rank_score\"], ascending=False)\n",
        "        .drop_duplicates(subset=[\"accession\"], keep=\"first\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    if steps_order is not None and not steps_order.empty:\n",
        "        top_by_step = top_by_step.merge(\n",
        "            steps_order.assign(_order=range(1, len(steps_order)+1)),\n",
        "            on=[\"pathway_tag\",\"step_idx\"], how=\"left\"\n",
        "        ).sort_values([\"_order\",\"rank_score\"], ascending=[True, False]).drop(columns=[\"_order\"]).reset_index(drop=True)\n",
        "\n",
        "    ranked_csv    = finish_out / \"sequences_ranked_by_step.csv\"\n",
        "    shortlist_csv = finish_out / \"sequences_shortlist.csv\"\n",
        "    top_by_step.to_csv(ranked_csv, index=False)\n",
        "    shortlist.to_csv(shortlist_csv, index=False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"sequences_ranked_csv\": str(ranked_csv),\n",
        "        \"sequences_shortlist_csv\": str(shortlist_csv),\n",
        "        \"sequence_rank_method\": \"score_norm + bacteria + preferred_host + acc_quality - blacklist\",\n",
        "        \"sequence_rank_counts\": {\n",
        "            \"steps\": int(top_by_step[[\"pathway_tag\",\"step_idx\"]].drop_duplicates().shape[0]),\n",
        "            \"unique_accessions\": int(shortlist.shape[0]),\n",
        "            \"rows_emitted\": int(top_by_step.shape[0]),\n",
        "            \"top_n_per_step\": int(TOP_N),\n",
        "        }\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"sequence_rank: ranked {top_by_step.shape[0]} rows across {new_state['sequence_rank_counts']['steps']} steps\",\n",
        "        f\"sequence_rank: unique shortlisted accessions = {shortlist.shape[0]}\",\n",
        "        f\"sequence_rank: outputs={ranked_csv}, {shortlist_csv}\"\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def doe_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Build a DoE screening sheet + markdown brief for the top-ranked pathway.\n",
        "    Also merges the top-ranked sequence per step (if available).\n",
        "    \"\"\"\n",
        "    from urllib.parse import quote_plus\n",
        "\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    rank_csv  = state.get(\"ranked_csv\") or state.get(\"ranked_final_csv\")\n",
        "    steps_csv = state.get(\"steps_plan_csv\") or state.get(\"steps_annotated_csv\") or str(finish_out / \"steps_enzyme_plan.csv\")\n",
        "    if not (rank_csv and Path(rank_csv).exists()):\n",
        "        raise FileNotFoundError(\"doe_node: ranked_csv missing or not found.\")\n",
        "    if not (steps_csv and Path(steps_csv).exists()):\n",
        "        raise FileNotFoundError(\"doe_node: steps CSV missing or not found.\")\n",
        "\n",
        "    ranked = pd.read_csv(rank_csv)\n",
        "    steps  = pd.read_csv(steps_csv)\n",
        "    ranked.columns = [c.strip() for c in ranked.columns]\n",
        "    steps.columns  = [c.strip() for c in steps.columns]\n",
        "\n",
        "    best_tag = str(ranked.iloc[0][\"pathway_tag\"])\n",
        "    psteps = (\n",
        "        steps[steps[\"pathway_tag\"] == best_tag]\n",
        "        .sort_values(\"step_idx\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # bring in top-ranked sequence per step (optional)\n",
        "    seq_csv = state.get(\"sequences_ranked_csv\")\n",
        "    merged_n = 0\n",
        "    top_seq_by_step = None\n",
        "    if seq_csv and Path(seq_csv).exists():\n",
        "        seq = pd.read_csv(seq_csv)\n",
        "        seq.columns = [c.strip() for c in seq.columns]\n",
        "        if \"pathway_tag\" in seq.columns and \"rank_score\" in seq.columns:\n",
        "            seq_best = seq[seq[\"pathway_tag\"] == best_tag].copy()\n",
        "            if not seq_best.empty:\n",
        "                seq_best[\"rank_score\"] = pd.to_numeric(seq_best[\"rank_score\"], errors=\"coerce\").fillna(0.0)\n",
        "                seq_best = (\n",
        "                    seq_best.sort_values([\"step_idx\",\"rank_score\"], ascending=[True, False])\n",
        "                            .groupby(\"step_idx\", as_index=False)\n",
        "                            .first()\n",
        "                )\n",
        "                keep_cols = {\n",
        "                    \"step_idx\":\"step_idx\",\n",
        "                    \"accession\":\"sequence_accession\",\n",
        "                    \"organism\":\"source_organism\",\n",
        "                    \"uniprot_url\":\"uniprot_url\"\n",
        "                }\n",
        "                for k in keep_cols.keys():\n",
        "                    if k not in seq_best.columns:\n",
        "                        seq_best[k] = \"\"\n",
        "                top_seq_by_step = seq_best[list(keep_cols.keys())].rename(columns=keep_cols)\n",
        "\n",
        "    cols = [\n",
        "        \"pathway_tag\",\"step_idx\",\"retrobiocat_reaction\",\"reaction_smiles\",\n",
        "        \"selected_enzyme\",\"possible_enzymes\",\"selenzyme_url\",\n",
        "        \"sequence_accession\",\"source_organism\",\"uniprot_url\",\n",
        "        \"assay_buffer\",\"pH\",\"temp_C\",\"cofactor\",\"cofactor_recycle\",\n",
        "        \"substrate_conc_mM\",\"enzyme_loading_mg_per_mL\",\"time_h\",\n",
        "        \"conversion_%\",\"notes\"\n",
        "    ]\n",
        "    rows = []\n",
        "    for _, r in psteps.iterrows():\n",
        "        query = quote_plus(str(r.get(\"selected_enzyme\") or r.get(\"retrobiocat_reaction\")))\n",
        "        uniprot_search = f\"https://www.uniprot.org/uniprotkb?query={query}\"\n",
        "        rows.append({\n",
        "            \"pathway_tag\": best_tag,\n",
        "            \"step_idx\": int(r.get(\"step_idx\", 0)),\n",
        "            \"retrobiocat_reaction\": r.get(\"retrobiocat_reaction\", \"\"),\n",
        "            \"reaction_smiles\": r.get(\"reaction_smiles\", \"\"),\n",
        "            \"selected_enzyme\": r.get(\"selected_enzyme\", \"\"),\n",
        "            \"possible_enzymes\": r.get(\"possible_enzymes\", \"\"),\n",
        "            \"selenzyme_url\": r.get(\"selenzyme_url\", \"\"),\n",
        "            \"sequence_accession\": \"\",\n",
        "            \"source_organism\": \"\",\n",
        "            \"uniprot_url\": uniprot_search,\n",
        "            \"assay_buffer\": \"\",\n",
        "            \"pH\": \"\",\n",
        "            \"temp_C\": \"\",\n",
        "            \"cofactor\": \"\",\n",
        "            \"cofactor_recycle\": \"\",\n",
        "            \"substrate_conc_mM\": \"\",\n",
        "            \"enzyme_loading_mg_per_mL\": \"\",\n",
        "            \"time_h\": \"\",\n",
        "            \"conversion_%\": \"\",\n",
        "            \"notes\": \"\",\n",
        "        })\n",
        "    df_screen = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "    if top_seq_by_step is not None and not top_seq_by_step.empty:\n",
        "        before_na = df_screen[\"sequence_accession\"].isna().sum() + (df_screen[\"sequence_accession\"] == \"\").sum()\n",
        "        df_screen = df_screen.merge(top_seq_by_step, on=\"step_idx\", how=\"left\", suffixes=(\"\",\"_best\"))\n",
        "        for col in [\"sequence_accession\",\"source_organism\",\"uniprot_url\"]:\n",
        "            best_col = f\"{col}_best\"\n",
        "            if best_col in df_screen.columns:\n",
        "                df_screen[col] = df_screen[best_col].where(df_screen[best_col].notna() & (df_screen[best_col]!=\"\"), df_screen[col])\n",
        "                df_screen.drop(columns=[best_col], inplace=True)\n",
        "        after_na = df_screen[\"sequence_accession\"].isna().sum() + (df_screen[\"sequence_accession\"] == \"\").sum()\n",
        "        merged_n = max(0, before_na - after_na)\n",
        "\n",
        "    screen_csv = str(finish_out / \"enzyme_screening_sheet.csv\")\n",
        "    df_screen.to_csv(screen_csv, index=False)\n",
        "\n",
        "    md_path = str(finish_out / \"pathway_brief.md\")\n",
        "    lines = []\n",
        "    lines.append(f\"# Pathway {best_tag} — Enzyme Screening Summary\\n\")\n",
        "    lines.append(f\"**Total Steps:** {len(psteps)}\\n\")\n",
        "    lines.append(f\"**Ranking File:** `{Path(rank_csv).name}`\\n\")\n",
        "\n",
        "    def bullet(txt): return f\"- {txt}\"\n",
        "\n",
        "    for _, r in psteps.iterrows():\n",
        "        lines.append(f\"\\n## Step {int(r['step_idx'])}: {r.get('retrobiocat_reaction','')}\\n\")\n",
        "        lines.append(bullet(f\"Reaction SMILES: `{r.get('reaction_smiles','')}`\"))\n",
        "        lines.append(bullet(f\"Selected enzyme: {r.get('selected_enzyme','-')}\"))\n",
        "        lines.append(bullet(f\"Possible enzymes: {r.get('possible_enzymes','-')}\"))\n",
        "        if \"precedent_best_similarity\" in psteps.columns and pd.notna(r.get(\"precedent_best_similarity\")):\n",
        "            lines.append(bullet(f\"Precedent similarity: {r.get('precedent_best_similarity')}\"))\n",
        "        if r.get(\"selenzyme_url\"):\n",
        "            lines.append(bullet(f\"[Selenzyme link]({r.get('selenzyme_url')})\"))\n",
        "        if merged_n and \"sequence_accession\" in df_screen.columns:\n",
        "            acc = df_screen.loc[df_screen[\"step_idx\"]==int(r[\"step_idx\"]),\"sequence_accession\"].values[0]\n",
        "            org = df_screen.loc[df_screen[\"step_idx\"]==int(r[\"step_idx\"]),\"source_organism\"].values[0]\n",
        "            up  = df_screen.loc[df_screen[\"step_idx\"]==int(r[\"step_idx\"]),\"uniprot_url\"].values[0]\n",
        "            if acc:\n",
        "                lines.append(bullet(f\"Top sequence: [{acc}]({up}) ({org})\"))\n",
        "        else:\n",
        "            if r.get(\"selected_enzyme\"):\n",
        "                q = quote_plus(r.get(\"selected_enzyme\"))\n",
        "                lines.append(bullet(f\"[UniProt search](https://www.uniprot.org/uniprotkb?query={q})\"))\n",
        "\n",
        "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"screening_sheet_csv\": screen_csv,\n",
        "        \"pathway_brief_md\": md_path,\n",
        "        \"doe_pathway_tag\": best_tag,\n",
        "        \"doe_steps\": len(psteps),\n",
        "        \"doe_sequences_merged\": int(merged_n),\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"DoE sheet + brief ready for {best_tag}\",\n",
        "        f\"screening={screen_csv}\",\n",
        "        f\"brief={md_path}\",\n",
        "        f\"sequences merged into DoE: {merged_n}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def human_gate_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Pause-and-review checkpoint node. Writes a small summary file and\n",
        "    checks for an external 'signals.human_approved' flag.\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    summary = {\n",
        "        \"top_pathway\": state.get(\"doe_pathway_tag\"),\n",
        "        \"steps\": state.get(\"doe_steps\"),\n",
        "        \"thermo_method\": state.get(\"thermo_method\"),\n",
        "        \"sequence_rank_counts\": state.get(\"sequence_rank_counts\", {}),\n",
        "        \"screening_sheet\": state.get(\"screening_sheet_csv\"),\n",
        "        \"brief\": state.get(\"pathway_brief_md\"),\n",
        "    }\n",
        "\n",
        "    gate_path = finish_out / \"_HUMAN_GATE.md\"\n",
        "    with open(gate_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Human Approval Gate\\n\\n\")\n",
        "        f.write(\"Please review the current design-build-test-learn outputs:\\n\\n\")\n",
        "        for k, v in summary.items():\n",
        "            f.write(f\"- **{k}**: {v}\\n\")\n",
        "        f.write(\"\\nMark `approved=True` in the next LangGraph signal to continue.\\n\")\n",
        "\n",
        "    signals = state.get(\"signals\", {})\n",
        "    approved = bool(signals.get(\"human_approved\")) or state.get(\"approved\", False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"approved\": approved,\n",
        "        \"human_gate_md\": str(gate_path),\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"human_gate: approval file ready at {gate_path}\",\n",
        "        f\"human_gate: approved={approved}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def export_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    Export all key DBTL artifacts and write a manifest.json.\n",
        "    \"\"\"\n",
        "    workdir = Path(state[\"workdir\"])\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    export_dir = workdir / \"retro_export\"\n",
        "    export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    artifacts = []\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, str) and any(v.endswith(ext) for ext in [\".csv\",\".md\",\".json\",\".xlsx\"]):\n",
        "            if os.path.exists(v):\n",
        "                dst = export_dir / Path(v).name\n",
        "                try:\n",
        "                    shutil.copy2(v, dst)\n",
        "                    artifacts.append(str(dst))\n",
        "                except Exception as e:\n",
        "                    state.setdefault(\"logs\", []).append(f\"export_node: failed to copy {v}: {e}\")\n",
        "\n",
        "    manifest = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"export_dir\": str(export_dir),\n",
        "        \"artifacts\": artifacts,\n",
        "        \"meta\": {\n",
        "            \"pathway_tag\": state.get(\"doe_pathway_tag\"),\n",
        "            \"approved\": state.get(\"approved\"),\n",
        "            \"thermo_method\": state.get(\"thermo_method\"),\n",
        "        },\n",
        "    }\n",
        "    manifest_path = export_dir / \"manifest.json\"\n",
        "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"export_manifest\": str(manifest_path),\n",
        "        \"export_dir\": str(export_dir),\n",
        "    }\n",
        "    new_state[\"logs\"] = state.get(\"logs\", []) + [\n",
        "        f\"export_node: exported {len(artifacts)} artifacts\",\n",
        "        f\"export_node: manifest={manifest_path}\",\n",
        "    ]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def build_node(state: DBTLState) -> DBTLState:\n",
        "    \"\"\"\n",
        "    (Optional) BUILD phase stub:\n",
        "    Query UniProt for quick candidates from selected enzyme names.\n",
        "    Writes build_out/sequence_candidates.csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import requests\n",
        "    except Exception:\n",
        "        # If no internet or requests missing, just noop gracefully.\n",
        "        new_state = {**state, **log(state, \"build: requests not available or offline; skipped\") }\n",
        "        return new_state\n",
        "\n",
        "    from urllib.parse import quote_plus\n",
        "\n",
        "    steps_csv = state.get(\"steps_plan_csv\")\n",
        "    if not steps_csv or not os.path.exists(steps_csv):\n",
        "        return {**state, **log(state, \"build: no steps_plan_csv; skipped\")}\n",
        "\n",
        "    out_dir   = Path(state[\"workdir\"]) / \"build_out\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    steps = pd.read_csv(steps_csv)\n",
        "    seq_rows = []\n",
        "    for _, r in steps.iterrows():\n",
        "        enzyme = str(r.get(\"selected_enzyme\", \"\")).strip()\n",
        "        if not enzyme or enzyme == \"nan\":\n",
        "            continue\n",
        "        q = quote_plus(enzyme)\n",
        "        uniprot_url = f\"https://rest.uniprot.org/uniprotkb/search?query={q}&format=tsv&fields=accession,organism,protein_name,length\"\n",
        "        try:\n",
        "            res = requests.get(uniprot_url, timeout=10)\n",
        "            if res.ok:\n",
        "                lines = res.text.splitlines()\n",
        "                if len(lines) > 1:\n",
        "                    parts = lines[1].split(\"\\t\")\n",
        "                    if len(parts) >= 4:\n",
        "                        acc, org, name, length = parts[0], parts[1], parts[2], parts[3]\n",
        "                        seq_rows.append({\n",
        "                            \"step_idx\": int(r[\"step_idx\"]),\n",
        "                            \"selected_enzyme\": enzyme,\n",
        "                            \"sequence_accession\": acc,\n",
        "                            \"source_organism\": org,\n",
        "                            \"uniprot_url\": f\"https://www.uniprot.org/uniprotkb/{acc}\",\n",
        "                            \"protein_name\": name,\n",
        "                            \"length\": length,\n",
        "                        })\n",
        "        except Exception as e:\n",
        "            seq_rows.append({\"step_idx\": r[\"step_idx\"], \"selected_enzyme\": enzyme, \"error\": str(e)})\n",
        "\n",
        "    seq_df = pd.DataFrame(seq_rows)\n",
        "    seq_csv = out_dir / \"sequence_candidates.csv\"\n",
        "    seq_df.to_csv(seq_csv, index=False)\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"build_out_dir\": str(out_dir),\n",
        "        \"sequence_csv\": str(seq_csv),\n",
        "        **log(state, f\"build: found {len(seq_rows)} sequence candidates\")\n",
        "    }\n",
        "    return new_state\n",
        "\n",
        "def simulate_node(state: \"DBTLState\") -> \"DBTLState\":\n",
        "    \"\"\"\n",
        "    Simulate pathway feasibility (toy COBRApy flux demo + mass-balance check).\n",
        "    Writes:\n",
        "      - retro_finish_out/simulation_fluxes.csv\n",
        "      - retro_finish_out/simulation_log.txt\n",
        "    Falls back gracefully if COBRApy is unavailable.\n",
        "    \"\"\"\n",
        "    import os, subprocess, textwrap, json, shutil, time\n",
        "    from pathlib import Path\n",
        "    from string import Template\n",
        "\n",
        "    def _log(s, msg):\n",
        "        s = {**s}\n",
        "        s[\"logs\"] = [*s.get(\"logs\", []), msg]\n",
        "        return s\n",
        "\n",
        "    workdir = Path(state[\"workdir\"]).expanduser().resolve()\n",
        "    retro_out = workdir / \"retro_out\"\n",
        "    finish_out = workdir / \"retro_finish_out\"\n",
        "    finish_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Inputs\n",
        "    rank_csv  = state.get(\"ranked_final_csv\") or state.get(\"ranked_csv\")\n",
        "    steps_csv = state.get(\"steps_annotated_csv\") or state.get(\"steps_plan_csv\")\n",
        "    if not rank_csv or not Path(rank_csv).exists():\n",
        "        return _log(state, \"simulate: no ranking CSV; skipped\")\n",
        "    if not steps_csv or not Path(steps_csv).exists():\n",
        "        return _log(state, \"simulate: no steps CSV; skipped\")\n",
        "\n",
        "    # Outputs\n",
        "    out_flux = finish_out / \"simulation_fluxes.csv\"\n",
        "    log_path = finish_out / \"simulation_log.txt\"\n",
        "\n",
        "    # Use the same micromamba env as RBC2\n",
        "    mm_root    = workdir / \"micromamba\"\n",
        "    env_prefix = mm_root / \"envs\" / \"retrobiocat\"\n",
        "    micromamba = workdir / \"bin\" / \"micromamba\"\n",
        "    if not micromamba.exists():\n",
        "        micromamba = \"micromamba\"\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env[\"MAMBA_ROOT_PREFIX\"] = str(mm_root)\n",
        "\n",
        "    # Ensure cobra is present (best-effort)\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            f\"\"\"{micromamba} run -p \"{env_prefix}\" python -c \"import cobra\" \"\"\",\n",
        "            shell=True, env=env, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        "        )\n",
        "        cobra_ok = True\n",
        "    except subprocess.CalledProcessError:\n",
        "        cobra_ok = False\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                f\"\"\"{micromamba} run -p \"{env_prefix}\" python -m pip install cobra\"\"\",\n",
        "                shell=True, env=env, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        "            )\n",
        "            cobra_ok = True\n",
        "        except subprocess.CalledProcessError:\n",
        "            cobra_ok = False\n",
        "\n",
        "    # ---- Embedded script (Template with $placeholders to avoid {} issues) ----\n",
        "    tpl = Template(r\"\"\"\n",
        "import pandas as pd, math, csv, sys\n",
        "from pathlib import Path\n",
        "\n",
        "RANK_CSV = Path(\"$rank_csv\")\n",
        "STEPS_CSV = Path(\"$steps_csv\")\n",
        "OUT_FLUX = Path(\"$out_flux\")\n",
        "LOG_PATH = Path(\"$log_path\")\n",
        "\n",
        "log_lines = []\n",
        "\n",
        "def log(x):\n",
        "    log_lines.append(str(x))\n",
        "\n",
        "ranked = pd.read_csv(RANK_CSV)\n",
        "steps  = pd.read_csv(STEPS_CSV)\n",
        "best_tag = str(ranked.iloc[0][\"pathway_tag\"])\n",
        "\n",
        "psteps = (steps[steps[\"pathway_tag\"] == best_tag]\n",
        "          .sort_values(\"step_idx\")\n",
        "          .reset_index(drop=True))\n",
        "\n",
        "# Basic mass-balance parse: count participants LHS/RHS for sanity\n",
        "def split_rxn(r):\n",
        "    if not isinstance(r, str) or \">>\" not in r: return [], []\n",
        "    L,R = r.split(\">>\",1)\n",
        "    subs=[s for s in L.split(\".\") if s]; prods=[p for p in R.split(\".\") if p]\n",
        "    return subs, prods\n",
        "\n",
        "imbalances = []\n",
        "for _, row in psteps.iterrows():\n",
        "    subs, prods = split_rxn(str(row.get(\"reaction_smiles\",\"\")))\n",
        "    imbalances.append(abs(len(subs) - len(prods)))\n",
        "mass_ok = (sum(imbalances) == 0)\n",
        "log(f\"mass_balance_ok={mass_ok} (total_imbalance={sum(imbalances)})\")\n",
        "\n",
        "# If COBRApy is available, create a toy model and assign unit flux per step\n",
        "try:\n",
        "    import cobra\n",
        "    model = cobra.Model(\"toy_pathway\")\n",
        "    # Make exchange for overall substrate mix and product pool\n",
        "    exch_in  = cobra.Reaction(\"EX_subs\")\n",
        "    exch_out = cobra.Reaction(\"EX_prod\")\n",
        "    model.add_reactions([exch_in, exch_out])\n",
        "    exch_in.lower_bound  = -1000.0; exch_in.upper_bound  = 0.0\n",
        "    exch_out.lower_bound = 0.0;     exch_out.upper_bound = 1000.0\n",
        "\n",
        "    # create a metabolite per unique token just to wire reactions\n",
        "    tokens = set()\n",
        "    for rxn in psteps[\"reaction_smiles\"].astype(str):\n",
        "        L,R = split_rxn(rxn)\n",
        "        tokens.update(L); tokens.update(R)\n",
        "    # Minimal placeholders; we don't use stoichiometries beyond 1\n",
        "    mets = {}\n",
        "    for t in tokens:\n",
        "        m_id = (\"m_\" + \"\".join([c if c.isalnum() else \"_\" for c in t]))[:60]\n",
        "        mets[t] = cobra.Metabolite(id=m_id, name=t, compartment=\"c\")\n",
        "\n",
        "    # Add step reactions (each consumes its LHS tokens and produces RHS tokens)\n",
        "    step_flux = []\n",
        "    for _, row in psteps.iterrows():\n",
        "        sid = f\"R_step_{int(row['step_idx'])}\"\n",
        "        rxn = cobra.Reaction(sid)\n",
        "        L,R = split_rxn(str(row.get(\"reaction_smiles\",\"\")))\n",
        "        rxn.lower_bound = 0.0; rxn.upper_bound = 1000.0\n",
        "        rxn.add_metabolites({ mets[s]: -1.0 for s in L if s in mets })\n",
        "        rxn.add_metabolites({ mets[p]:  1.0 for p in R if p in mets })\n",
        "        model.add_reactions([rxn])\n",
        "        step_flux.append((sid, 1.0))  # target steady-state flux guess\n",
        "\n",
        "    # Objective: push flux through the last step\n",
        "    if step_flux:\n",
        "        last_sid = step_flux[-1][0]\n",
        "        model.objective = last_sid\n",
        "\n",
        "    sol = model.optimize()\n",
        "    fluxes = []\n",
        "    if sol.status == \"optimal\":\n",
        "        for sid, _ in step_flux:\n",
        "            fluxes.append({\"pathway_tag\": best_tag,\n",
        "                           \"step_id\": sid,\n",
        "                           \"flux\": float(sol.fluxes.get(sid, 0.0))})\n",
        "        flux_ok = True\n",
        "    else:\n",
        "        flux_ok = False\n",
        "        for sid, _ in step_flux:\n",
        "            fluxes.append({\"pathway_tag\": best_tag, \"step_id\": sid, \"flux\": float(\"nan\")})\n",
        "\n",
        "    pd.DataFrame(fluxes).to_csv(OUT_FLUX, index=False)\n",
        "    log(f\"cobra_status={sol.status}, flux_ok={flux_ok}, rows={len(fluxes)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    # COBRA missing or failed — emit a stub CSV with zeros\n",
        "    fluxes = []\n",
        "    for _, row in psteps.iterrows():\n",
        "        fluxes.append({\"pathway_tag\": best_tag,\n",
        "                       \"step_id\": f\"R_step_{int(row['step_idx'])}\",\n",
        "                       \"flux\": 0.0})\n",
        "    pd.DataFrame(fluxes).to_csv(OUT_FLUX, index=False)\n",
        "    log(f\"cobra_unavailable_or_failed: {e}; wrote zero-flux stub\")\n",
        "\n",
        "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(log_lines))\n",
        "print(\"simulate: wrote\", OUT_FLUX, \"and\", LOG_PATH)\n",
        "\"\"\")\n",
        "\n",
        "    py = textwrap.dedent(tpl.substitute(\n",
        "        rank_csv=str(rank_csv),\n",
        "        steps_csv=str(steps_csv),\n",
        "        out_flux=str(out_flux),\n",
        "        log_path=str(log_path),\n",
        "    ))\n",
        "\n",
        "    # run inside env\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            f\"\"\"{micromamba} run -p \"{env_prefix}\" python - <<'PY'\\n{py}\\nPY\"\"\",\n",
        "            shell=True, env=env, check=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
        "        )\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # if even that fails, record and continue\n",
        "        with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\nsubprocess failed:\\n\" + (e.stdout or \"\"))\n",
        "\n",
        "    new_state = {\n",
        "        **state,\n",
        "        \"simulation_flux_csv\": str(out_flux) if out_flux.exists() else None,\n",
        "        \"simulation_log\": str(log_path) if log_path.exists() else None,\n",
        "    }\n",
        "    new_state[\"logs\"] = [*state.get(\"logs\", []), f\"simulate: fluxes={out_flux.exists()}, log={log_path.exists()}\"]\n",
        "    return new_state\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Wire the graph\n",
        "# =========================\n",
        "graph = StateGraph(DBTLState)\n",
        "\n",
        "# Nodes (unchanged)\n",
        "graph.add_node(\"retrosynthesis\", wrap_node(\"retrosynthesis\", retrosynthesis_node))\n",
        "graph.add_node(\"extract\",       wrap_node(\"extract\",       extract_pathways_node))\n",
        "graph.add_node(\"thermo\",        wrap_node(\"thermo\",        thermo_score_node))\n",
        "graph.add_node(\"rank\",          wrap_node(\"rank\",          rank_node))\n",
        "graph.add_node(\"simulate\",      wrap_node(\"simulate\",      simulate_node))\n",
        "graph.add_node(\"selenzyme\",     wrap_node(\"selenzyme\",     selenzyme_node))\n",
        "graph.add_node(\"seqrank\",       wrap_node(\"seqrank\",       sequence_rank_node))\n",
        "graph.add_node(\"doe\",           wrap_node(\"doe\",           doe_node))\n",
        "graph.add_node(\"build\",         wrap_node(\"build\",         build_node))\n",
        "graph.add_node(\"approve\",       wrap_node(\"approve\",       human_gate_node))\n",
        "graph.add_node(\"export\",        wrap_node(\"export\",        export_node))\n",
        "\n",
        "# Edges: simulate runs AFTER rank and BEFORE selenzyme.\n",
        "graph.set_entry_point(\"retrosynthesis\")\n",
        "graph.add_edge(\"retrosynthesis\",\"extract\")\n",
        "graph.add_edge(\"extract\",\"thermo\")\n",
        "graph.add_edge(\"thermo\",\"rank\")\n",
        "graph.add_edge(\"rank\",\"simulate\")\n",
        "graph.add_edge(\"simulate\",\"selenzyme\")\n",
        "graph.add_edge(\"selenzyme\",\"seqrank\")\n",
        "graph.add_edge(\"seqrank\",\"doe\")\n",
        "graph.add_edge(\"doe\",\"build\")\n",
        "graph.add_edge(\"build\",\"approve\")\n",
        "\n",
        "# Approve: either proceed to export or end\n",
        "graph.add_conditional_edges(\n",
        "    \"approve\",\n",
        "    lambda s: \"export\" if s.get(\"approved\") else \"__end__\",\n",
        "    {\"export\": \"export\", \"__end__\": END},\n",
        ")\n",
        "graph.add_edge(\"export\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Example run\n",
        "# =========================\n",
        "# init_state = DBTLState(\n",
        "#     run_id=\"run_001\",\n",
        "#    workdir=\"/content/runs/run_001\",\n",
        "#     target_smiles=\"COC1=C(C=CC(=C1)C=O)O\",\n",
        "#     host=\"Escherichia coli\",\n",
        "#     constraints={\"max_steps\":\"5\"},\n",
        "#     logs=[]\n",
        "# )\n",
        "# for state in app.stream(init_state, stream_mode=\"values\"):\n",
        "#     node   = state.get(\"last_node\", \"?\")\n",
        "#     status = state.get(\"status\", \"?\")\n",
        "#     print(f\"[{node}] {status}\")\n",
        "# print(\"Final logs:\")\n",
        "# print(\"\\n\".join(state.get(\"logs\", [])))\n"
      ],
      "metadata": {
        "id": "2Io9R2Q4iWoj"
      },
      "id": "2Io9R2Q4iWoj",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"DBTL_QUIET\"] = \"1\"   # silence micromamba/pip/stdout\n",
        "\n",
        "\n",
        "init_state = {\n",
        "  \"run_id\": \"run_001\",\n",
        "  \"workdir\": \"/content/runs/run_001\",\n",
        "  \"target_smiles\": \"COC1=C(C=CC(=C1)C=O)O\",\n",
        "  \"host\": \"Escherichia coli\",\n",
        "  \"constraints\": {\"max_steps\": \"5\"},\n",
        "  \"signals\": {\"human_approved\": True},   # ← approve\n",
        "  \"approved\": True,\n",
        "  \"logs\": []\n",
        "}\n",
        "\n",
        "for state in app.stream(init_state, stream_mode=\"values\"):\n",
        "    pass\n",
        "\n",
        "print(\"Export manifest:\", state.get(\"export_manifest\"))\n",
        "print(\"Export dir:\", state.get(\"export_dir\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCTurJShp6wa",
        "outputId": "e1c57e75-941f-4431-b6c1-2a70b2d1e167"
      },
      "id": "kCTurJShp6wa",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export manifest: /content/runs/run_001/retro_export/manifest.json\n",
            "Export dir: /content/runs/run_001/retro_export\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}